<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Inter-rater agreement and LLMs, continued</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../assets/favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-753e6901f2c274e22bcfe03890f74e5f.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-CES9J82GLL"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-CES9J82GLL', { 'anonymize_ip': true});
</script>
<!--<script async src=”https://siteimproveanalytics.com/js/siteanalyze_6294756.js”></script>-->

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../assets/styles.css">
</head>

<body class="quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#coding" id="toc-coding" class="nav-link active" data-scroll-target="#coding">Coding</a></li>
  <li><a href="#inter-rater-agreement" id="toc-inter-rater-agreement" class="nav-link" data-scroll-target="#inter-rater-agreement">Inter-rater agreement</a></li>
  <li><a href="#cohens-kappa" id="toc-cohens-kappa" class="nav-link" data-scroll-target="#cohens-kappa">Cohen’s Kappa</a>
  <ul class="collapse">
  <li><a href="#cohens-kappa-formula" id="toc-cohens-kappa-formula" class="nav-link" data-scroll-target="#cohens-kappa-formula">Cohen’s Kappa Formula</a></li>
  <li><a href="#cohens-kappa-as-a-measure-of-inter-rater-agreement" id="toc-cohens-kappa-as-a-measure-of-inter-rater-agreement" class="nav-link" data-scroll-target="#cohens-kappa-as-a-measure-of-inter-rater-agreement">Cohen’s Kappa as a Measure of Inter-Rater Agreement</a></li>
  <li><a href="#high-agreement-implies-more-reliability" id="toc-high-agreement-implies-more-reliability" class="nav-link" data-scroll-target="#high-agreement-implies-more-reliability">High agreement implies more reliability</a></li>
  </ul></li>
  <li><a href="#example-binary-classification-with-grant-decisions" id="toc-example-binary-classification-with-grant-decisions" class="nav-link" data-scroll-target="#example-binary-classification-with-grant-decisions">Example: Binary Classification with Grant Decisions</a>
  <ul class="collapse">
  <li><a href="#compute-observed-agreement-rate-p_o" id="toc-compute-observed-agreement-rate-p_o" class="nav-link" data-scroll-target="#compute-observed-agreement-rate-p_o">Compute observed agreement rate, <span class="math inline">\(p_o\)</span></a></li>
  <li><a href="#compute-random-agreement-rate-p_e" id="toc-compute-random-agreement-rate-p_e" class="nav-link" data-scroll-target="#compute-random-agreement-rate-p_e">Compute random agreement rate, <span class="math inline">\(p_e\)</span></a></li>
  <li><a href="#compute-cohens-kappa-kappa" id="toc-compute-cohens-kappa-kappa" class="nav-link" data-scroll-target="#compute-cohens-kappa-kappa">Compute Cohen’s Kappa, <span class="math inline">\(\kappa\)</span></a></li>
  <li><a href="#determine-agreement-level" id="toc-determine-agreement-level" class="nav-link" data-scroll-target="#determine-agreement-level">Determine Agreement Level</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Inter-rater agreement and LLMs, continued</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="coding" class="level2">
<h2 class="anchored" data-anchor-id="coding">Coding</h2>
<p>The previous lecture discussed the idea of <strong>qualitative coding</strong>, central to analysis of open-ended text and other qualitative data.</p>
<p><strong>Coding</strong>: The process of translating written or visual material into standardized categories. Codes are the labels/tags for chunks of text, <em>not</em> the programming/coding we’ve been doing so far.</p>
<ol type="1">
<li>First, determine codebook. This is the set of category labels for the data.</li>
<li>Then, <strong>“code”</strong> the data. Apply codes (i.e., labels) to text.</li>
</ol>
</section>
<section id="inter-rater-agreement" class="level2">
<h2 class="anchored" data-anchor-id="inter-rater-agreement">Inter-rater agreement</h2>
<p>However, the process of coding has historically needed to be completed (at least in part) by humans (i.e., <strong>raters</strong>).</p>
<p>It’s likely that if codes for a very large dataset were generated by a single rater, there might be risks to <strong>validity</strong>. After all, perhaps this rater would have certain preconceptions that encoded their way into labels. If codes for a very large dataset were generated by multiple human raters, there might be risks to <strong>reliability</strong>, because different raters may code the same datapoint differently.</p>
<p>In practice, multiple researchers collaborate to assign codes that are both reliable and valid:</p>
<ol type="1">
<li>Co-design codebook.</li>
<li>Then, work together by labeling a subset of the data separately, then coming together to discuss agreement. This assesses reliability of labels.</li>
<li>If they don’t agree, then they revisit the codebook and variable definitions, then try coding again. This also revisits the validity of coding process, potentially redefining how the concept is operationalized by the new codes (i.e., categories) in the variable codebook.</li>
<li>Repeat this process until reasonable levels of agreement are reached.</li>
<li>Then, independently code the rest of the dataset.</li>
</ol>
<p>How do researchers reach “reasonable levels of agreement”? One common measure is Cohen’s Kappa.</p>
</section>
<section id="cohens-kappa" class="level2">
<h2 class="anchored" data-anchor-id="cohens-kappa">Cohen’s Kappa</h2>
<p><strong>Cohen’s Kappa</strong> is a measure of agreement between two annotators (or raters) who independently classify items into categories.</p>
<p>Unlike simple agreement rates (e.g., how often the two raters agree), <strong>Cohen’s Kappa adjusts for chance agreement,</strong> that is, how often two people might agree just by random guessing.</p>
<section id="cohens-kappa-formula" class="level3">
<h3 class="anchored" data-anchor-id="cohens-kappa-formula">Cohen’s Kappa Formula</h3>
<p>Cohen’s kappa measures the agreement between two raters who each classify items into a set of mutually exclusive categories. Here is the mathematical notation of Cohen’s Kappa, denoted by the greek letter kappa (<span class="math inline">\(\kappa\)</span>):</p>
<p><span class="math display">\[\kappa = \frac{p_o - p_e}{1 - p_e},\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(p_o\)</span> = observed agreement rate, i.e., how often the raters agreed</li>
<li><span class="math inline">\(p_e\)</span> = random agreement rate, i.e., how likely the raters would agree just by randomly guessing.</li>
</ul>
<p>We will not go into the probability in this course, but here’s the idea. Cohen’s Kappa is a <strong>ratio</strong> of two values:</p>
<p><span class="math display">\[\kappa = \dfrac{\text{observed agreement rate} - \text{random agreement rate}}{1 - \text{random agreement rate}}\]</span></p>
<p>If the raters are in complete agreement, then the observed agreement rate <span class="math inline">\(p_o = 1\)</span> and <span class="math inline">\(\kappa = 1\)</span>. If the raters only agree randomly, then the observed agreement rate <span class="math inline">\(p_o = p_e\)</span> and <span class="math inline">\(\kappa = 0\)</span>. It is possible for <span class="math inline">\(\kappa &lt; 0\)</span>, which can occur if there is really no relationship between the raters’ rankings, or if raters are biased in their ratings.</p>
</section>
<section id="cohens-kappa-as-a-measure-of-inter-rater-agreement" class="level3">
<h3 class="anchored" data-anchor-id="cohens-kappa-as-a-measure-of-inter-rater-agreement">Cohen’s Kappa as a Measure of Inter-Rater Agreement</h3>
<p>Values of Cohen’s Kappa can be used to determine <strong>inter-rater agreement</strong>: i.e., how closely two raters agree. There are no agreed upon thresholds in the literature, but Landis and Koch (1981, <a href="https://doi.org/10.2307/2529310">DOI</a>) is the one used in the Ziems et al.&nbsp;(2024, <a href="https://doi.org/10.1162/coli_a_00502">DOI</a>) we study in this course.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><span class="math inline">\(\kappa\)</span></th>
<th>Agreement</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>&lt; 0</td>
<td>no agreement</td>
</tr>
<tr class="even">
<td>0-0.2</td>
<td>poor</td>
</tr>
<tr class="odd">
<td>0.21-0.4</td>
<td>fair</td>
</tr>
<tr class="even">
<td>0.41-0.6</td>
<td>moderate</td>
</tr>
<tr class="odd">
<td>0.61 - 0.8</td>
<td>good</td>
</tr>
<tr class="even">
<td>0.8 - 1.0</td>
<td>near-perfect agreement</td>
</tr>
</tbody>
</table>
<p>Again—these categories should be seen as just guidelines. In fact, Landis and Koch supplied no evidence to support it, basing it instead on personal opinion.</p>
</section>
<section id="high-agreement-implies-more-reliability" class="level3">
<h3 class="anchored" data-anchor-id="high-agreement-implies-more-reliability">High agreement implies more reliability</h3>
<p>Inter-rater agreement can also provide a measure of <strong>reliability</strong>. If we find that agreement levels are quite high among two raters, then it is likely that we can rely on the labels provided by these raters to do further analysis. By contrast, if we find that agreement levels are quite poor, then we cannot rely on the labels provided by the raters. In this case, we search for other means of labeling/coding the data, or we revisit our codebook/label categories and training process for labeling data.</p>
</section>
</section>
<section id="example-binary-classification-with-grant-decisions" class="level2">
<h2 class="anchored" data-anchor-id="example-binary-classification-with-grant-decisions">Example: Binary Classification with Grant Decisions</h2>
<p><strong>In general, we will </strong>not** ask you to manually compute Cohen’s Kappa**. We will see that there is a convenient Python library called sklearn for computing Cohen’s Kappa in practice. However, it is good to first internalize this idea of “random chance” with the manual computation below. You will see the library implementation in lab.</p>
<p>In this task, we will manually compute Cohen’s Kappa on a binary labeling task. This question just uses very simple Python (like a calculator!), but make sure you understand how we compute each part of the Cohen’s Kappa formula. This example is adapted from the <a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa">Wikipedia article on Cohen’s Kappa</a>.</p>
<p>Suppose that you were analyzing data related to a group of 50 people applying for a grant, each of whom submitted a grant proposal. Each grant proposal was read by a panel of two readers and each reader decides either “Yes” or “No” to the proposal. Suppose the summary of readers A and B’s decisions were as follows:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>B: Yes</th>
<th>B: No</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><b>A: Yes</b></td>
<td>20</td>
<td>5</td>
</tr>
<tr class="even">
<td><b>A: No</b></td>
<td>10</td>
<td>15</td>
</tr>
</tbody>
</table>
<p>This means:</p>
<ul>
<li>Both A and B agreed on 35 grants:
<ul>
<li>Both said <strong>Yes</strong> on 20 grants.</li>
<li>Both said <strong>No</strong> on 15 grants.</li>
</ul></li>
<li>A and B disagreed on 15 grants:
<ul>
<li>A said <strong>Yes</strong>, B said <strong>No</strong> on 5 grants.</li>
<li>A said <strong>No</strong>, B said <strong>Yes</strong> on 10 grants.</li>
</ul></li>
</ul>
<section id="compute-observed-agreement-rate-p_o" class="level3">
<h3 class="anchored" data-anchor-id="compute-observed-agreement-rate-p_o">Compute observed agreement rate, <span class="math inline">\(p_o\)</span></h3>
<p>The rate of observed agreement <span class="math inline">\(p_o\)</span> is the fraction of grants for which A and B actually agreed on their decision, i.e., they both decided “yes” or both decided “no”.</p>
<p>This rate is:</p>
<p><span class="math display">\[ p_o = \frac{20 + 15}{50} = 0.7\]</span></p>
</section>
<section id="compute-random-agreement-rate-p_e" class="level3">
<h3 class="anchored" data-anchor-id="compute-random-agreement-rate-p_e">Compute random agreement rate, <span class="math inline">\(p_e\)</span></h3>
<p>The rate of random agreement <span class="math inline">\(p_e\)</span> is the <em>hypothetical</em> (i.e., expected) fraction of grants for which A and B might randomly agree. That is, if A had randomly voted yes on agreements <em>based on A’s observed “yes” rate</em>, and B had randomly also voted yes on agreements <em>based on B’s observed “yes” rate</em>, then sometimes A and B might agree in their decisions purely based on chance.</p>
<p>We demonstrate an algorithm to compute <span class="math inline">\(p_e\)</span> below. The precise formula for this rate is rooted in probability, which you will cover in a future probability and statistics class. But you can imagine that “random agreement” of A and B is like flipping two coins and seeing the rate at which both land on heads or both land on tails.</p>
<ol type="1">
<li>Compute the observed “yes” rates of A and B.
<ul>
<li>Reader A said “Yes” to 25 applicants and “No” to 25 applicants. Thus reader A said “Yes” 50% of the time.</li>
<li>Reader B said “Yes” to 30 applicants and “No” to 20 applicants. Thus reader B said “Yes” 60% of the time.</li>
</ul></li>
<li>Compute the probability that both A and B would say “yes” at random. If reader A says “Yes” 50% randomly, and reader B says “Yes” 60% randomly, then this probability is <span class="math inline">\(0.5 \times 0.6 = 0.3\)</span>.</li>
<li>Compute the probability that both A and B would say “no” at random. If reader A says “Yes” 50% randomly then they otherwise say “No” 50% randomly; similarly, if reader B says “Yes” 60% randomly then they otherwise say “No” 40% randomly. Therefore this probability that both say “no” is <span class="math inline">\((1 - 0.5) \times (1 - 0.6) = 0.5 \times 0.4 = 0.2\)</span>.</li>
<li>The probability that A and B agree is the sum of these two probabilities. <span class="math inline">\(0.3 + 0.2 = 0.5\)</span>.</li>
</ol>
</section>
<section id="compute-cohens-kappa-kappa" class="level3">
<h3 class="anchored" data-anchor-id="compute-cohens-kappa-kappa">Compute Cohen’s Kappa, <span class="math inline">\(\kappa\)</span></h3>
<p>Finally, we use these values of <span class="math inline">\(p_o\)</span> and <span class="math inline">\(p_e\)</span> to compute Cohen’s Kappa</p>
<p><span class="math display">\[\kappa = \frac{p_o - p_e}{1 - p_e}\]</span></p>
<p>Again, we have included the text description of this formula in case it is easier to work through:</p>
<p><span class="math display">\[\kappa = \dfrac{\text{observed agreement rate} - \text{random agreement rate}}{1 - \text{random agreement rate}}\]</span></p>
<p>As per the <a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa#Simple_example">Wikipedia article</a>, this value should be:</p>
<p><span class="math display">\[ \kappa = \frac{0.7 - 0.5}{1 - 0.5} = 0.4.\]</span></p>
</section>
<section id="determine-agreement-level" class="level3">
<h3 class="anchored" data-anchor-id="determine-agreement-level">Determine Agreement Level</h3>
<p>Based on this value of <span class="math inline">\(\kappa\)</span> and the thresholds provided by Landis and Koch, we would determine that <span class="math inline">\(\kappa = 0.4\)</span> provides a <strong>fair</strong> level of agreement.</p>
<p>What do we do with this evaluation? It depends on our application. If we were trying to get a general sense of how many grants were approved by this panel, this level of agreement might be fine.</p>
<p>On the other hand, if we were deciding whether to fund grant proposals based on the decisions of raters and B, this might not be a high enough threshold of agreement for us to determine that the ratings were reliable. We may want to ask the raters to revisit their ratings, or rediscuss aspects of what makes a good grant application.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../index.html">
<p>Home / About / License</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://github.com/data-6-berkeley">
<p>GitHub</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://dap.berkeley.edu/get-help/report-web-accessibility-issue">
<p>Accessibility</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://ophd.berkeley.edu/policies-and-procedures/nondiscrimination-policy-statement">
<p>Nondiscrimination</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>