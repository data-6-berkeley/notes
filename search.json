[
  {
    "objectID": "03-arrays/numpy.html",
    "href": "03-arrays/numpy.html",
    "title": "NumPy and Array Functions",
    "section": "",
    "text": "This note has the following goals:",
    "crumbs": [
      "L03: Arrays",
      "NumPy and Array Functions"
    ]
  },
  {
    "objectID": "03-arrays/numpy.html#array-functions",
    "href": "03-arrays/numpy.html#array-functions",
    "title": "NumPy and Array Functions",
    "section": "Array Functions",
    "text": "Array Functions\nAs data scientists we may find it useful to perform operations on arrays beyond simple element-wise arithmetic operations. We can do so via a range of functions from different sources.\nHere are some starter arrays to get us going. What arrays are created?\n\nfrom datascience import *\n\nint_arr = make_array(3, -4, 0, 5, 2)\nstr_arr = make_array(\"cm\", \"m\", \"in\", \"ft\", \"yd\")\nempty_arr = make_array() # challenge\n\n\n\n\n\n\n\nArrays created\n\n\n\n\n\n\nint_arr\n\narray([ 3, -4,  0,  5,  2])\n\n\n\nstr_arr\n\narray(['cm', 'm', 'in', 'ft', 'yd'],\n      dtype='&lt;U2')\n\n\nAn empty array is one with no elements:\n\nempty_arr\n\narray([], dtype=float64)\n\n\n\n\n\nWe will describe the syntax and terminology of built-in and NumPy functions below, then provide a table of operations. Here’s how we suggest working through this section:\n\nDon’t memorize these functions!\nInstead, remember there are two types of functions for arrays: built-in functions and NumPy functions.\nGet familiar with two tables, copied below. Get familiar with reading this tabl documentation to understand how each function works.\nWhen writing your programs, look through these functions and see which function(s) can compose your solution.\n\n\nBuilt-in functions\nSome built-in functions (i.e., included with Python) can take in arrays as arguments.\nHere is a table of example built-in functions for arrays. Again, don’t memorize the functions. Rather, get familiar with reading and predicting their outputs.\n\nBuilt-in Python functions that take in array arguments.\n\n\n\n\n\n\nExpression and Return value\nExample(s)\n\n\n\n\nlen(arr) Length of an array, providing the number of elements it contains. Useful for determining the size of an array dynamically.\nlen(str_arr)   # 5  len(empty_arr) # 0\n\n\nmax(arr) The largest value within an array.\nmax(int_arr)    # 5  max(str_arr)   # 'yd'\n\n\nmin(arr) The smallest value within an array.\nmin(int_arr)\n\n\nsum(arr) Sum of all values in an array.\nsum(int_arr)   # 6  sum(str_arr)   # TypeError\n\n\n\n\n\n\n\n\n\nNote 1: Argument data types\n\n\n\n\n\nRemember that even though function names can be identical, call expressions can evaluate differently depending on the argument data type. len(arg), for example, returns an integer value indicating the “length” of the argument arg. If arg is a string, it returns the number of characters; if arg is an array, it returns the number of elements.\n\nlen(str_arr)\n\n5\n\n\n\nlen(str_arr.item(0)) # what is the argument here?\n\n2\n\n\n\n\n\n\n\n\n\n\n\nNote 2: Expressions with string arrays\n\n\n\n\n\nAs you may have noticed above, calling these functions on strings return some seemingly bizarre values. After all, what does it mean to get the maximum value of an array of strings?\nInstead of erroring out, the Pythonic convention is to consider alphabetic sorting as a way of ordering elements—hence, \"yd\" comes alphabetically after \"cm\" and \"ft\", and so on.\nWe will not cover string comparisons in detail in this course. If you are curious about these algorithms, we encourage you to take a Data Structures course!\n\n\n\nLet’s use these built-in functions to compute the average (mean) value of the below array. We will discuss measures of average much later in this course.\n\narr = make_array(30, -40, -4.5, 0, 35)\navg = sum(arr)/len(arr)\navg\n\n4.0999999999999996\n\n\nDue to approximations in how the computer stores and operates on floats, the above number is as close to 4.1 (the true numeric average) as we can get with our Python calculator. Take a computer systems or computer architecture course for more information!\n\n\nNumPy functions\nNumPy (pronounced “NUM-pie”) is a Python library with convenient and powerful modules and functions for manipulating arrays. Any time we want to use NumPy, we must write an import statement:\n\nimport numpy as np\n\narr = make_array(30, -40, -4.5, 0, 35)\narr\n\narray([ 30. , -40. ,  -4.5,   0. ,  35. ])\n\n\nAfter putting this statement at the top of our notebook, we can then prepend np. to call a NumPy function. The below NumPy function call computes averages much more conveniently than our clever (but verbose) expression with built-in function calls, though it still suffers from floating point approximations:\n\nnp.average(arr)\n\n4.0999999999999996\n\n\n\n\nNumPy Array Function Table\nThere are many, many types of NumPy array functions; the below table only scratches the surface of what is possible. Again, instead of memorizing functions, we encourage you to learn how to read documentation by considering the following:\n\nWhat is the function name? How does this name inform the function description?\nWhat is the return value of this function?\n\nIf the data type of the return value is an array, is it the same length as the original array? Is the function therefore operating element-wise on the original array?\nIf the data type of the return value is a single value, how is this value computed from the different elements of the original array?\n\nIs the function changing the contents of the original array?\n\n\nA subset of NumPy array functions. A full reference is in the Data 6 reference sheet.\n\n\n\n\n\n\nNumPy function\nDescription\n\n\n\n\nnp.average(arr)  np.mean(arr)\nThe average (i.e., mean) value of arr\n\n\nnp.sum(arr)\nThe sum of all elements in arr\n\n\nnp.prod(arr)\nThe product of all elements in arr\n\n\nnp.count_nonzero(arr)\nThe number of elements in arr that are not equal to 0\n\n\nnp.diff(arr)\nThe difference between each element and the previous one value of arr. Returns an array of length 1 less than the original.\n\n\nnp.cumsum(arr)\nThe cumulative sum of all elements in arr.\n\n\nnp.sqrt(arr)\nThe square roots of each element in arr.\n\n\nnp.log(arr)\nThe natural logarithm of each element in arr.\n\n\nnp.log10(arr)\nThe base-10 logarithm of each element in arr.\n\n\nnp.sort(arr)\nSort the elements in arr.\n\n\n\nRefer to the lecture notebook for example call expressions involving these NumPy functions. Refer to the Data 6 reference sheet for all functions we will expect you to be familiar with (not memorize!) in this course.",
    "crumbs": [
      "L03: Arrays",
      "NumPy and Array Functions"
    ]
  },
  {
    "objectID": "03-arrays/numpy.html#external-reading",
    "href": "03-arrays/numpy.html#external-reading",
    "title": "NumPy and Array Functions",
    "section": "External Reading",
    "text": "External Reading\n\n(mentioned in notes) Computational and Inferential Thinking, Ch 5.1\n(optional) Tomas Beuzen. Python Programming for Data Science Ch 1.2.",
    "crumbs": [
      "L03: Arrays",
      "NumPy and Array Functions"
    ]
  },
  {
    "objectID": "03-arrays/numpy.html#references",
    "href": "03-arrays/numpy.html#references",
    "title": "NumPy and Array Functions",
    "section": "References",
    "text": "References\nU.S. Census Bureau, “EDUCATIONAL ATTAINMENT,” American Community Survey 5-Year Estimates Subject Tables, Table S1501, 2020, https://data.census.gov/table/ACSST5Y2020.S1501?q=2020+education&t=Age+and+Sex:Educational+Attainment&g=010XX00US$0400000, accessed on August 24, 2025.",
    "crumbs": [
      "L03: Arrays",
      "NumPy and Array Functions"
    ]
  },
  {
    "objectID": "five-things.html",
    "href": "five-things.html",
    "title": "Five things you need to know to pass this class",
    "section": "",
    "text": "Here they are, in no particular order.",
    "crumbs": [
      "5 Things to Know"
    ]
  },
  {
    "objectID": "five-things.html#read-instructions-carefully",
    "href": "five-things.html#read-instructions-carefully",
    "title": "Five things you need to know to pass this class",
    "section": "1) Read Instructions Carefully",
    "text": "1) Read Instructions Carefully\nYou must carefully read the instructions provided for each assignment. Read, not skim! They contain information vital for the completion of the assigned work.",
    "crumbs": [
      "5 Things to Know"
    ]
  },
  {
    "objectID": "five-things.html#do-all-the-assigned-work",
    "href": "five-things.html#do-all-the-assigned-work",
    "title": "Five things you need to know to pass this class",
    "section": "2) Do all the assigned work",
    "text": "2) Do all the assigned work\nThis course is a very hands-on course and requires many hours of practical work outside class and lab. It also requires reviewing ALL the learning materials shared on this website and on the course website.\nIt goes without saying that you should do all the assigned work: attend lecture and section, review the practice material discussed in lab, and of course do all HW assignments. Keep in mind that the activities and assignments build upon earlier work. So it’s important not to fall behind and avoid leaving gaps along the semester.\nOn the technical side, you should have your own computer, (good) internet connection, and also know how to record a video of both 1) computer’s screen capture, and 2) face capture (e.g. a zoom recording makes this easy). We will provide more detailed instructions about the required tools as we move forward with the semester.",
    "crumbs": [
      "5 Things to Know"
    ]
  },
  {
    "objectID": "five-things.html#no-need-to-memorize-all-commands",
    "href": "five-things.html#no-need-to-memorize-all-commands",
    "title": "Five things you need to know to pass this class",
    "section": "3) No need to memorize all commands",
    "text": "3) No need to memorize all commands\nDo you need to memorize all commands? No! We don’t expect that you memorize all commands. In fact, you can find a series of cheatsheets that you can (and should) use at all times (even during quizzes and tests). We will release these soon.\nHowever, we do expect that you learn the most common types of functions in Python and Jupyter notebooks: e.g. print(), ?, etc. More important, we expect that you understand the “logic” and working principles of certain data objects, common programming structures, good practices, etc.",
    "crumbs": [
      "5 Things to Know"
    ]
  },
  {
    "objectID": "five-things.html#study-for-tests",
    "href": "five-things.html#study-for-tests",
    "title": "Five things you need to know to pass this class",
    "section": "4) Study for tests",
    "text": "4) Study for tests\nThe exams are a way to test your understanding of the various concepts presented in the course. The exams are also a way to test whether you are really doing all the practical work by yourself.\nIn theory, students who do an honest effort in completing all the assignments (e.g. writing commands, understanding commands, learning the syntax, etc) should be able to get a passing score in these tests.",
    "crumbs": [
      "5 Things to Know"
    ]
  },
  {
    "objectID": "five-things.html#what-else-do-you-recommend-to-succeed-in-this-course",
    "href": "five-things.html#what-else-do-you-recommend-to-succeed-in-this-course",
    "title": "Five things you need to know to pass this class",
    "section": "5) What else do you recommend to succeed in this course?",
    "text": "5) What else do you recommend to succeed in this course?\nThis one is hard to answer, in part because it depends on your personal definition of “success”. Simply put, I don’t think there’s a unique recipe for success. Instead, let me answer this question by telling you about the typical factors that may negatively affect your performance:\n\nnot attending lecture and/or lab,\nnot submitting assignments,\nlooking at the solutions of other students and “inadvertently” copy them,\npoor studying/working habits\nbeing afraid/scared/ashamed of asking the teaching staff for help\nyou’ve been doing work of passing quality and you cannot complete the course due to circumstances beyond your control\n\nDon’t underestimate the second to last item. Coding (in any programming language) can be extremely frustrating at times. You would be surprised to hear my collection of student stories about all sorts of bugs, typos, misspellings, and the like, that gave them a fair amount of frustration. So please, ask the teaching staff for help in a timely and respectful manner.\nAs for the last item, please let us know you’ve been affected by circumstances beyond your control as soon as possible. While we cannot guarantee any outcome, we will do what is within our reach to help you in this class.",
    "crumbs": [
      "5 Things to Know"
    ]
  },
  {
    "objectID": "five-things.html#acknowledgments",
    "href": "five-things.html#acknowledgments",
    "title": "Five things you need to know to pass this class",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThe original version of this text was written by Gaston Sanchez for Stat 133 Spring 2025.",
    "crumbs": [
      "5 Things to Know"
    ]
  },
  {
    "objectID": "02-datatypes/rates-incidence.html",
    "href": "02-datatypes/rates-incidence.html",
    "title": "Case Study: Rates",
    "section": "",
    "text": "How can Python help us understand numerical data? Let’s explore a case study in the context of a public health data report!",
    "crumbs": [
      "L02: Data Types",
      "Application: Rates and Incidence"
    ]
  },
  {
    "objectID": "02-datatypes/rates-incidence.html#definitions",
    "href": "02-datatypes/rates-incidence.html#definitions",
    "title": "Case Study: Rates",
    "section": "Definitions",
    "text": "Definitions\nBefore we dive into the case study, let’s describe some key data science terms: tables and rates.\n\nTables\nIn this course, we will describe many examples of data structures, which are ways to store and organize data, often for computational processing. One of the most common examples is a table:\n\nA rectangular data structure composed of rows and columns. Columns are labeled.\n\nWe will expand on this definition of table over the next few lectures.\n\n\nRates\nA rate is a measure of one quantity per unit of some other quantity. A few examples: 6 miles per hour, 4.8 parts per million. Rates are often expressed as a percentage, fraction with a numerator and a denominator, or a decimal number.\nData scientists often use and define rates to compare different situations or events. For example, it is easy to compare 60 mph to 40 mph, but harder to compare 15 mph to 10 km/s. Occasionally, it’s unclear why a rate is necessary until we dig into the data. Let’s inspect a particular case below.",
    "crumbs": [
      "L02: Data Types",
      "Application: Rates and Incidence"
    ]
  },
  {
    "objectID": "02-datatypes/rates-incidence.html#rates-incidence",
    "href": "02-datatypes/rates-incidence.html#rates-incidence",
    "title": "Case Study: Rates",
    "section": "Rates: Incidence",
    "text": "Rates: Incidence\nThe U.S. Center for Disease Control (CDC) regularly examines disease data nationwide and publishes reports for the public. These reports help inform public health policy and consequent responses to disease epidemics at the national and global levels. One such disease is Tuberculosis (TB), a highly contagious respiratory infection.\nConsider the reported U.S. TB cases in 2021 (CDC Morbidity and Mortality Weekly Report (MMWR) 03/25/2022, source). The report summary states:\n\nReported TB incidence (cases per 100,000 persons) increased 9.4%, from 2.2 during 2020 to 2.4 during 2021 but was lower than incidence during 2019 (2.7). Increases occurred among both U.S.-born and non–U.S.-born persons.\n\nWhile the report discusses possible interpretations as to why this occurred, let’s focus on this particular numeric summary by answering the following questions:\n\nDefine: What is incidence? Incidence is a rate. Why use this rate for comparison, and not the total number of cases?\nVerify: Consider Table 1. How can we use Python to verify that the TB incidence column can be computed from the TB cases column? How can use Python to verify the reported percent change in incidence?\n\n\n\n\n\n\n\nLook at the tabular data\n\n\n\nThere is a lot of information on the CDC website—more than we can cover in this example. Our main goal is to understand the numeric data presented in the summary statement quoted above. There are two sources of information relevant to us:\n\nThe quote above, taken from the webpage summary.\nTable 1, located at the bottom of the webpage.\n\nBefore continuing, try reading Table 1. Hint: Read the footnotes!",
    "crumbs": [
      "L02: Data Types",
      "Application: Rates and Incidence"
    ]
  },
  {
    "objectID": "02-datatypes/rates-incidence.html#define-incidence",
    "href": "02-datatypes/rates-incidence.html#define-incidence",
    "title": "Case Study: Rates",
    "section": "Define: Incidence",
    "text": "Define: Incidence\nIn epidemiology, incidence is a rate that measures the number of cases of a disease in a population, within a given time period, as shown in Equation 1:\n\\[ \\text{TB incidence} = \\frac{\\text{\\# TB cases}}{\\text{population}} \\tag{1}\\]\nThis rate can be interpreted as the number of cases (i.e., the number of reported individuals diagnosed with TB) per person (the number of people in the population).\nWhy not use TB cases? Why scale by population size? Based on the CDC summary, the intention of this report was to highlight a drop in TB cases in 2020, compared to adjacent years. The report accomplished this by reporting and comparing the TB incidence in 2019, 2020, and 2021.\nSimply reporting the total number of TB cases has several pitfalls, including the inability to compare the prevalence of TB in different scenarios.\nWhile the precise explanation requires a strong understanding of probability (see Data 140), intuitively, incidence is a proxy for the prevalence, or rate of occurrence, of TB cases occurring in a population. As an example, Hawaii reported much fewer TB cases than California, but had a much higher TB incidence, across all three years. After all, Hawaii has a much smaller population than California (1.5 million vs. 39 million), so each case of reported TB matters more.\n\nNext, let’s consider how the CDC defines TB incidence in this report. From the Table 1 footnote:\n\nCases per 100,000 persons using midyear population estimates from the U.S. Census Bureau.\n\nIncorrect interpretation. First, consider the following (incorrect) ratio in Equation 2:\n\\[ \\frac{\\text{\\# TB cases}}{100,000 \\text{ persons}} \\tag{2}\\]\nThis rate does not account for different states having different population sizes. In other words, the population denominator in Equation 1 has disappeared.\nCorrect interpretation. Incidence is a measure of the rate of occurrence of a disease across a population. The Table 1 footnote translates Equation 1 to consider different ways of measuring the population.\nIf TB incidence in Equation 1 was defined as number of TB cases per person, we scale up by 100,000 to define TB cases per group, where the group is defined as 100,000 persons. \\[\n\\frac{\\text{cases}}{\\text{1 person}} \\times \\frac{100,000 \\text{ persons}}{\\text{group}}\n\\]\nThe CDC definition of TB incidence is therefore represented by Equation 3:\n\\[\n\\frac{\\text{\\# TB cases}}{\\# \\text{people in population}} \\times 100,000\n\\tag{3}\\]",
    "crumbs": [
      "L02: Data Types",
      "Application: Rates and Incidence"
    ]
  },
  {
    "objectID": "02-datatypes/rates-incidence.html#verifying-incidence",
    "href": "02-datatypes/rates-incidence.html#verifying-incidence",
    "title": "Case Study: Rates",
    "section": "Verifying Incidence",
    "text": "Verifying Incidence\nTo compute TB incidence, we need to source two pieces of data: the number of TB cases from Table 1, and the midyear population estimates from the U.S. Census. For your convenience, we’ve included a few data points on each U.S. jurisdiction in the two tables below (click to expand).\n\n\n\n\n\n\nSnippet of Table 1, U.S. CDC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo. of TB cases\nTB incidence\n\n\nU.S. jurisdiction\n2019\n2020\n2021\n2019\n2020\n2021\n\n\n\n\nTotal\n8,900\n7,173\n7,860\n2.71\n2.16\n2.37\n\n\nAlabama\n87\n72\n92\n1.77\n1.43\n1.83\n\n\nAlaska\n58\n58\n58\n7.91\n7.92\n7.92\n\n\nArizona\n183\n136\n129\n2.51\n1.89\n1.77\n\n\nArkansas\n64\n59\n69\n2.12\n1.96\n2.28\n\n\nCalifornia\n2,111\n1,706\n1,750\n5.35\n4.32\n4.46\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n\n\n\n\n\n\n\n\n\n\nSnippet of midyear population estimates, U.S. Census Bureau\n\n\n\n\n\nHere are the population estimates for the corresponding years, sourced from the U.S. Census Bureau:\n\n\n\n\n\n\n\n\n\nU.S. jurisdiction\n2019 pop\n2020 pop\n2021 pop\n\n\n\n\nTotal\n328,239,523\n331,501,080\n331,893,745\n\n\nAlabama\n4,903,185\n5,024,803\n5,039,877\n\n\nCalifornia\n39,512,223\n39,499,738\n39,237,836\n\n\n\n\n\n\nLet’s use Equation 3 to compute TB incidence in the U.S. in 2020:\n\n7173 / 331501080 * 100000\n\n2.1637938555132306\n\n\nRounding to the nearest tenth spot, we get the original quoted rate, 2.2!\nThe previous cell makes little sense without the prior intense exposition. Python names and comments make everything more understandable:\n\n# compute incidence as cases per 100k\npop_2020 = 331501080\ntb_2020 = 7173\nincidence_2020 = tb_2020 / pop_2020 * 100000\nincidence_2020\n\n2.1637938555132306\n\n\nIf we need to verify all incidences in the 2020 column, you might be tempted to edit the cell above, manually inputting the values for each U.S. jurisdiction. This approach is both tedious and error-prone (what if you input incorrect numbers); in a few lectures, we will show you a much easier way using data structures: arrays and tables.",
    "crumbs": [
      "L02: Data Types",
      "Application: Rates and Incidence"
    ]
  },
  {
    "objectID": "02-datatypes/rates-incidence.html#references",
    "href": "02-datatypes/rates-incidence.html#references",
    "title": "Case Study: Rates",
    "section": "References",
    "text": "References\nFilardo TD, Feng P, Pratt RH, Price SF, Self JL. Tuberculosis — United States, 2021. MMWR Morb Mortal Wkly Rep 2022;71:441–446. DOI: http://dx.doi.org/10.15585/mmwr.mm7112a1\nU.S. Census, 2024. State Population Totals and Components of Change: 2010-2019, https://www.census.gov/data/tables/time-series/demo/popest/2010s-state-total.html\nU.S. Census, 2025. State Population Totals and Components of Change: 2020-2024. https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html.",
    "crumbs": [
      "L02: Data Types",
      "Application: Rates and Incidence"
    ]
  },
  {
    "objectID": "staff.html",
    "href": "staff.html",
    "title": "Staff",
    "section": "",
    "text": "Office hours are subject to change.\nYou can attend OH of any staff member.\n\n\n\nInstructor: Gaston Sanchez gastonsanchez@berkeley.edu Office hours:  - Fri 4:10pm-5:10pm, Evans 309\n\n\nGSI: Calvin Carter calv2n@berkeley.edu Office hours: - Mon 2pm-4pm, Evans 434 - Wed 12pm-2pm, Evans 434\n\n\nGSI: Huong Vu huong_vu@berkeley.edu Office hours:  - Tue 3-5pm, Evans 434 - Thr 3-5pm, Evans 434\n\n\nGSI: Dylan Webb dylancw@berkeley.edu Office hours:  - Wed 2-5pm, Evans 434 - Fri 2-5pm, on Zoom\n\n\nTutor: David Sun davidzhongyisun@berkeley.edu Office hours:  - Tue 12:30-3pm, Evans 428 - Thr 1-3pm, Evans 446\n\n\nTutor: Evan Passalacqua evanqua@berkeley.edu Office hours:  - Mon 9-11am, Evans 426 - Wed 9-11am, Evans 426"
  },
  {
    "objectID": "04-tables/exercises.html",
    "href": "04-tables/exercises.html",
    "title": "Table Exercises",
    "section": "",
    "text": "We work with the schools table from before.\nfrom datascience import *\nimport numpy as np\n\nschools = Table.read_table('data/cal_unis.csv')\nschools.show(3)\n\n\n\n\nName\nInstitution\nCity\nCounty\nEnrollment\nFounded\n\n\n\n\nCalifornia State Polytechnic University, Humboldt\nCSU\nArcata\nHumboldt\n6025\n1913\n\n\nCalifornia State University, Bakersfield\nCSU\nBakersfield\nKern\n9613\n1965\n\n\nUniversity of California, Berkeley\nUC\nBerkeley\nAlameda\n45307\n1869\n\n\n\n\n... (29 rows omitted)\nPull up the Python Reference and try these exercises yourself! They are roughly in order of increasing difficulty.",
    "crumbs": [
      "L04: Table Fundamentals",
      "Table Exercises"
    ]
  },
  {
    "objectID": "04-tables/exercises.html#exercise-1-variable-names",
    "href": "04-tables/exercises.html#exercise-1-variable-names",
    "title": "Table Exercises",
    "section": "Exercise 1: Variable names",
    "text": "Exercise 1: Variable names\nHow do we get all the column labels of schools?\n\n\n\n\n\n\nShow Answer\n\n\n\n\n\n\nschools.labels\n\n('Name', 'Institution', 'City', 'County', 'Enrollment', 'Founded')",
    "crumbs": [
      "L04: Table Fundamentals",
      "Table Exercises"
    ]
  },
  {
    "objectID": "04-tables/exercises.html#exercise-2-reorder-columns",
    "href": "04-tables/exercises.html#exercise-2-reorder-columns",
    "title": "Table Exercises",
    "section": "Exercise 2: Reorder columns",
    "text": "Exercise 2: Reorder columns\nHow do we reorder the columns, as below?\n\n\n\n\n\n\n\n\n\n\n\nName\nFounded\nInstitution\nCity\nCounty\nEnrollment\n\n\n\n\n…\n…\n…\n…\n…\n…\n\n\n\nHint: use one of select, drop, or with_columns.\n\n\n\n\n\n\nShow Answer\n\n\n\n\n\n\nschools.select(\n  \"Name\", \"Founded\",\n  \"Institution\", \"City\",\n  \"County\", \"Enrollment\"\n)\n\n\n\n\nName\nFounded\nInstitution\nCity\nCounty\nEnrollment\n\n\n\n\nCalifornia State Polytechnic University, Humboldt\n1913\nCSU\nArcata\nHumboldt\n6025\n\n\nCalifornia State University, Bakersfield\n1965\nCSU\nBakersfield\nKern\n9613\n\n\nUniversity of California, Berkeley\n1869\nUC\nBerkeley\nAlameda\n45307\n\n\nCalifornia State University Channel Islands\n2002\nCSU\nCamarillo\nVentura\n6128\n\n\nCalifornia State University, Dominguez Hills\n1960\nCSU\nCarson\nLos Angeles\n16426\n\n\nCalifornia State University, Chico\n1887\nCSU\nChico\nButte\n14183\n\n\nUniversity of California, Davis\n1905\nUC\nDavis\nYolo\n39679\n\n\nCalifornia State University, Fresno\n1911\nCSU\nFresno\nFresno\n23999\n\n\nCalifornia State University, Fullerton\n1957\nCSU\nFullerton\nOrange\n40386\n\n\nCalifornia State University, East Bay\n1957\nCSU\nHayward\nAlameda\n13673\n\n\n\n\n... (22 rows omitted)",
    "crumbs": [
      "L04: Table Fundamentals",
      "Table Exercises"
    ]
  },
  {
    "objectID": "04-tables/exercises.html#exercise-3-filtering",
    "href": "04-tables/exercises.html#exercise-3-filtering",
    "title": "Table Exercises",
    "section": "Exercise 3: Filtering",
    "text": "Exercise 3: Filtering\n\n\n\n\n\n\nThe schools table for reference, if you need it\n\n\n\n\n\n\nschools.show(3)\n\n\n\n\nName\nInstitution\nCity\nCounty\nEnrollment\nFounded\n\n\n\n\nCalifornia State Polytechnic University, Humboldt\nCSU\nArcata\nHumboldt\n6025\n1913\n\n\nCalifornia State University, Bakersfield\nCSU\nBakersfield\nKern\n9613\n1965\n\n\nUniversity of California, Berkeley\nUC\nBerkeley\nAlameda\n45307\n1869\n\n\n\n\n... (29 rows omitted)\n\n\n\n\n\n\nHow do we get a table with only UC schools?\n\n\n\n\n\n\n\nShow Answer\n\n\n\n\n\n\nschools.where(\"Institution\", \"UC\")\n\n\n\n\nName\nInstitution\nCity\nCounty\nEnrollment\nFounded\n\n\n\n\nUniversity of California, Berkeley\nUC\nBerkeley\nAlameda\n45307\n1869\n\n\nUniversity of California, Davis\nUC\nDavis\nYolo\n39679\n1905\n\n\nUniversity of California, Irvine\nUC\nIrvine\nOrange\n35937\n1965\n\n\nUniversity of California, Los Angeles\nUC\nLos Angeles\nLos Angeles\n46430\n1882\n\n\nUniversity of California, Merced\nUC\nMerced\nMerced\n9110\n2005\n\n\nUniversity of California, Riverside\nUC\nRiverside\nRiverside\n26809\n1954\n\n\nUniversity of California, San Diego\nUC\nSan Diego\nSan Diego\n42006\n1960\n\n\nUniversity of California, Santa Barbara\nUC\nSanta Barbara\nSanta Barbara\n26420\n1891\n\n\nUniversity of California, Santa Cruz\nUC\nSanta Cruz\nSanta Cruz\n19478\n1965\n\n\n\n\n\n\n\n\n\nHow do we get a table with all the schools in Los Angeles?\n\n\n\n\n\n\n\nShow Answer\n\n\n\n\n\n\nschools.where(\"City\", \"Los Angeles\")\n\n\n\n\nName\nInstitution\nCity\nCounty\nEnrollment\nFounded\n\n\n\n\nUniversity of California, Los Angeles\nUC\nLos Angeles\nLos Angeles\n46430\n1882\n\n\nCalifornia State University, Los Angeles\nCSU\nLos Angeles\nLos Angeles\n26460\n1947",
    "crumbs": [
      "L04: Table Fundamentals",
      "Table Exercises"
    ]
  },
  {
    "objectID": "04-tables/exercises.html#exercise-4-rename-columns",
    "href": "04-tables/exercises.html#exercise-4-rename-columns",
    "title": "Table Exercises",
    "section": "Exercise 4: Rename Columns",
    "text": "Exercise 4: Rename Columns\nHow do we update schools such that the column Name is renamed University? Hint: Check out the method relabeled.\n\n\n\n\n\n\nShow Answer\n\n\n\n\n\n\nschools = schools.relabeled(\"Name\", \"University\")\nschools.show(3)\n\n\n\n\nUniversity\nInstitution\nCity\nCounty\nEnrollment\nFounded\n\n\n\n\nCalifornia State Polytechnic University, Humboldt\nCSU\nArcata\nHumboldt\n6025\n1913\n\n\nCalifornia State University, Bakersfield\nCSU\nBakersfield\nKern\n9613\n1965\n\n\nUniversity of California, Berkeley\nUC\nBerkeley\nAlameda\n45307\n1869\n\n\n\n\n... (29 rows omitted)\n\n\n\n\n\nThere are many ways to approach a problem. Suppose you didn’t know the method relabeled existed:\n\n\n\n\n\n\nAlternate Answer\n\n\n\n\n\n\nschools = schools.with_column(\"University\", schools.column(\"Name\")).drop(\"Name\")\nschools.show(3)\n\n\n\n\nInstitution\nCity\nCounty\nEnrollment\nFounded\nUniversity\n\n\n\n\nCSU\nArcata\nHumboldt\n6025\n1913\nCalifornia State Polytechnic University, Humboldt\n\n\nCSU\nBakersfield\nKern\n9613\n1965\nCalifornia State University, Bakersfield\n\n\nUC\nBerkeley\nAlameda\n45307\n1869\nUniversity of California, Berkeley\n\n\n\n\n... (29 rows omitted)\n\n\nQuestions for you as you read the above code:\n\nWhat methods are being chained together here? Which is executed first, with_column or drop?\nWhen is the column method being called?\nWhat would happen if you switched the two methods? (try it out)\nWhy is University now the last column?",
    "crumbs": [
      "L04: Table Fundamentals",
      "Table Exercises"
    ]
  },
  {
    "objectID": "04-tables/exercises.html#exercise-5-debugging-show",
    "href": "04-tables/exercises.html#exercise-5-debugging-show",
    "title": "Table Exercises",
    "section": "Exercise 5: Debugging show",
    "text": "Exercise 5: Debugging show\nThe following cells exhibit a tricky, but potentially common bug. First, check out the Python Reference to understand what show does.\n\nschools = Table.read_table('data/cal_unis.csv')\n\n\nschools = schools.show(3)\n\n\n\n\nName\nInstitution\nCity\nCounty\nEnrollment\nFounded\n\n\n\n\nCalifornia State Polytechnic University, Humboldt\nCSU\nArcata\nHumboldt\n6025\n1913\n\n\nCalifornia State University, Bakersfield\nCSU\nBakersfield\nKern\n9613\n1965\n\n\nUniversity of California, Berkeley\nUC\nBerkeley\nAlameda\n45307\n1869\n\n\n\n\n... (29 rows omitted)\n\n\n\nschools = schools.show(4)\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 schools = schools.show(4)\n\nAttributeError: 'NoneType' object has no attribute 'show'\n\n\n\nLike print, show is for display purposes. It does not return anything—i.e., it returns None. The second cell above therefore assigns schools to None!\nDespite this, show—just like print—is useful when you want to display intermediate output for debugging purposes.\n\nschools = Table.read_table('data/cal_unis.csv')\nschools.show(3) # just the first three rows\nschools = schools.relabeled(\"Name\", \"University\")\nschools.show(3) # and again\nschools = schools.with_columns(\"Name\", \"City\")\nschools         # the last evaluated value in a cell\n                # is displayed by default\n\n\n\n\nName\nInstitution\nCity\nCounty\nEnrollment\nFounded\n\n\n\n\nCalifornia State Polytechnic University, Humboldt\nCSU\nArcata\nHumboldt\n6025\n1913\n\n\nCalifornia State University, Bakersfield\nCSU\nBakersfield\nKern\n9613\n1965\n\n\nUniversity of California, Berkeley\nUC\nBerkeley\nAlameda\n45307\n1869\n\n\n\n\n... (29 rows omitted)\n\n\n\n\n\nUniversity\nInstitution\nCity\nCounty\nEnrollment\nFounded\n\n\n\n\nCalifornia State Polytechnic University, Humboldt\nCSU\nArcata\nHumboldt\n6025\n1913\n\n\nCalifornia State University, Bakersfield\nCSU\nBakersfield\nKern\n9613\n1965\n\n\nUniversity of California, Berkeley\nUC\nBerkeley\nAlameda\n45307\n1869\n\n\n\n\n... (29 rows omitted)\n\n\n\n\n\nUniversity\nInstitution\nCity\nCounty\nEnrollment\nFounded\nName\n\n\n\n\nCalifornia State Polytechnic University, Humboldt\nCSU\nArcata\nHumboldt\n6025\n1913\nCity\n\n\nCalifornia State University, Bakersfield\nCSU\nBakersfield\nKern\n9613\n1965\nCity\n\n\nUniversity of California, Berkeley\nUC\nBerkeley\nAlameda\n45307\n1869\nCity\n\n\nCalifornia State University Channel Islands\nCSU\nCamarillo\nVentura\n6128\n2002\nCity\n\n\nCalifornia State University, Dominguez Hills\nCSU\nCarson\nLos Angeles\n16426\n1960\nCity\n\n\nCalifornia State University, Chico\nCSU\nChico\nButte\n14183\n1887\nCity\n\n\nUniversity of California, Davis\nUC\nDavis\nYolo\n39679\n1905\nCity\n\n\nCalifornia State University, Fresno\nCSU\nFresno\nFresno\n23999\n1911\nCity\n\n\nCalifornia State University, Fullerton\nCSU\nFullerton\nOrange\n40386\n1957\nCity\n\n\nCalifornia State University, East Bay\nCSU\nHayward\nAlameda\n13673\n1957\nCity\n\n\n\n\n... (22 rows omitted)",
    "crumbs": [
      "L04: Table Fundamentals",
      "Table Exercises"
    ]
  },
  {
    "objectID": "06-variables-ii/deprecated-eda.html",
    "href": "06-variables-ii/deprecated-eda.html",
    "title": "Exploratory Data Analysis - consider removing",
    "section": "",
    "text": "If we’re not going to be studying causal relationships in this class, what will we be looking at? In this course we will look deeply at a core component of Data Science: Exploratory Data Analysis, or EDA."
  },
  {
    "objectID": "06-variables-ii/deprecated-eda.html#what-is-eda",
    "href": "06-variables-ii/deprecated-eda.html#what-is-eda",
    "title": "Exploratory Data Analysis - consider removing",
    "section": "What is EDA?",
    "text": "What is EDA?\nExploratory Data Analysis (EDA) is like detective work. As coined by the famous American statistician and mathematician John Tukey (we will discuss Tukey numbers soon):\n\nExploratory data analysis is an attitude, a state of flexibility, a willingness to look for those things that we believe are not there, as well as those that we believe to be there.\n\nMore formally, Exploratory Data Analysis (EDA) is the process of understanding a new dataset. It is an open-ended, informal analysis that involves familiarizing ourselves with the variables present in the data, discovering potential hypotheses, and identifying possible issues with the data.\n\nWhat is Data Wrangling?\nA process very closely related to EDA is data wrangling, often called data cleaning. Data wrangling is the process of transforming raw data to facilitate subsequent analysis and can address issues like unclear structure or formatting, missing or corrupted values, unit conversions, and so on.\nEDA and data cleaning are often thought of as an “infinite loop,” with each process driving the other.\nFortunately, in our classes we will try our best to work with “clean” datasets. These datasets will often have already been preprocessed for cleaner analysis, allowing us to explore and ask questions much more easily than if we were stuck with messier data."
  },
  {
    "objectID": "06-variables-ii/deprecated-index.html",
    "href": "06-variables-ii/deprecated-index.html",
    "title": "Intro to Social Science Research",
    "section": "",
    "text": "Theory\nHypothesis\nData Collection\nData Analysis\nResults\n(and back)"
  },
  {
    "objectID": "06-variables-ii/deprecated-index.html#the-scientific-method",
    "href": "06-variables-ii/deprecated-index.html#the-scientific-method",
    "title": "Intro to Social Science Research",
    "section": "",
    "text": "Theory\nHypothesis\nData Collection\nData Analysis\nResults\n(and back)"
  },
  {
    "objectID": "06-variables-ii/deprecated-index.html#theory-vs.-hypothesis",
    "href": "06-variables-ii/deprecated-index.html#theory-vs.-hypothesis",
    "title": "Intro to Social Science Research",
    "section": "Theory vs. Hypothesis",
    "text": "Theory vs. Hypothesis\nTheory: A systematic set of related statements that accord with a worldview.\nWhen (social) scientists do research, we use theories 3 different ways: * Theories prevent us from being taken in by flukes. * Theories help us make sense of observed patterns in ways that can suggest other possibilities. * Theories direct our research efforts, pointing us toward likely discoveries through empirical observation.\nElements of a scientific sociological theory: * Concepts * Relations between concepts * Causal mechanisms\nHypothesis: The empirical instantiation of a theory, i.e., a testable statement of a relationship involving two concepts. * Hypotheses therefore involve variables, which are representations that capture the different dimensions, categories, or levels of a concept.\ninsert images of concept 1, concept 2, variable 1, variable 2"
  },
  {
    "objectID": "01-intro/programming-basics.html",
    "href": "01-intro/programming-basics.html",
    "title": "Programming Basics",
    "section": "",
    "text": "Let’s get programming!\nThe first goal of this lecture is to demonstrate that some Python programming is as familiar as using a calculator, which you have likely used before.\nThe second goal is to introduce programming terminology—expressions, names, assignments, data types, etc. Learning this terminology is like learning grammar; these terms help us describe precisely what our programs are doing. Getting a handle on the terminology earlier will make it easier to describe the programming concepts. But it does take some memorization…!",
    "crumbs": [
      "L01: Introduction",
      "Programming Basics"
    ]
  },
  {
    "objectID": "01-intro/programming-basics.html#expressions",
    "href": "01-intro/programming-basics.html#expressions",
    "title": "Programming Basics",
    "section": "Expressions",
    "text": "Expressions\n\n\n\n\n\n\nRead Inferential Thinking\n\n\n\nRead Ch 3.1, which describes in detail how Python evaluates numeric expressions.\nBefore continuing, make sure you understand the terminology:\n\nexpression\nevaluation\nsyntax\noperators\noperands\n\n\n\n\nSummary of Numeric Operators\n\nCommon Python operators for numeric data types\n\n\n\n\n\n\n\n\nOperator\nSymbol\nExample Expression\nExpression Value\n\n\n\n\nAddition\n+\n2 + 3\n5\n\n\nSubtraction\n-\n15 - 4\n11\n\n\nMultiplication\n*\n-2 * 9\n-18\n\n\nDivision\n/\n15 / 2\n7.5\n\n\nInteger division(Cuts off remainder)\n//\n15 // 2\n7\n\n\nRemainder/Modulo\n%\n19 % 3\n1  (19 ÷ 3 = 6 Remainder 1)\n\n\nExponentiation\n**\n3 ** 2\n9",
    "crumbs": [
      "L01: Introduction",
      "Programming Basics"
    ]
  },
  {
    "objectID": "01-intro/programming-basics.html#names-and-call-expressions",
    "href": "01-intro/programming-basics.html#names-and-call-expressions",
    "title": "Programming Basics",
    "section": "Names and Call Expressions",
    "text": "Names and Call Expressions\n\n\n\n\n\n\nRead Inferential Thinking\n\n\n\nRead Ch 3.2 and Ch 3.3, which define names and call expressions.\nBefore continuing, make sure you understand the following:\n\nIn Python, a name can be given to a value using an assignment statement (which involves the assignment operator, =).\nA function is a named operation.\nA call expression invokes functions: a function is called on arguments, the argument values are passed into the function; the function then returns the final value to the larger call expression.\nIn Jupyter Notebooks, placing a ? after a function name will bring up a built-in description of that function.\nMost functions in Python are stored in modules (e.g., math) that can be imported via import then called with a dot (e.g., math.log(10)).\n\nTerminology:\n\nname\nassignment statement\nfunction, arguments, return value\ncall expression\n\n\n\nOne analogy for names is suitcase tags. Consider the following assignment statement:\n\nx = 3\n\nThis statement assigns the name x to the value 3. Like a suitcase tag, the name x is bound to the value 3.\n\nx\n\n3\n\n\nPython first evaluates the expression on the right-hand side of the = assignment operator, then binds the name x to the resulting value.\nThe below statement re-assigns the name x. Think of this as moving the suitcase tag to a different suitcase.\n\nx = 1 + 2 * 3 - 4 // 5\nx\n\n7\n\n\n\nA note on function calls\nWe will use the term function call interchangeably with call expression. You will see why when we talk more about defining our own functions\n\n\n\n\n\n\nAssignment statements are not expressions!\n\n\n\nStatements, when executed, do something. Expressions are a type of statement that, when executed, evaluate to values. In Jupyter Notebook code cells, if the last statement in a cell is not an expression, executing that last statement will not produce an output.\nThe first cell above does not end in an expression and therefore does not produce output. The second cell above has two statements; the last one is an expression, which is displayed in the cell output.\n\n\nConsider the below Python code:\n\nx = 4\ny = max(-2, 9) + x\n\nPython executes these two statements sequentially: first, Line 1 is an assignment statement that assigns the name x to 4. Then, Line 2 assigns the name y to the result of evaluating the right-hand-side.",
    "crumbs": [
      "L01: Introduction",
      "Programming Basics"
    ]
  },
  {
    "objectID": "01-intro/programming-basics.html#style-and-debugging",
    "href": "01-intro/programming-basics.html#style-and-debugging",
    "title": "Programming Basics",
    "section": "Style and Debugging",
    "text": "Style and Debugging\nYou may have noticed by now that Jupyter Notebooks are not just by computers to run code. Data scientists also use notebooks to understand how the code facilitates data analysis. We’d therefore like to establish two habits early:\n\nProgramming Style and Comments\nProgramming style involves writing code that is self-evident and understandable by other human beings. It is not sufficient for your code to be functional, i.e., perform the correct computation. It should also be readable and interpretable, for others to reuse and adapt.\nGood style practices can involve comments, meaningful names, whitespace, markdown cells interspersed with code cells, etc. Ch 3.2 of Inferential Thinking describes meaningful names; we discuss comments below.\nComments are used to explain what code does. Good programmers write code that is self-evident and use comments only where necessary.\nIn Python, you can write comments in the same line as code (“in-line” comments) using #:\n\n3 + 4     # simple arithmetic\n\n7\n\n\nThe above shows that both code and comments can be on the same line. In that line, anything after # is a comment and is not evaluated.\n\n\nDebugging\nDebugging is the process of fixing errors, i.e., bugs, in code. Debugging is a huge process and can take up the majority of your coding time.\nErrors in your code can pop up for any number of reasons! Here are some tips:\nTest your code early and often. If you write a lot of code before testing, then you have that much more code to debug and check. Instead, write a bit, test and check, and keep writing. This will require you to know how to decompose, or break down, your solutions into multiple steps that you can test individually. Once you’re comfortable with one step, move onto the next step. That’s right—it’s abstraction!_ Debugging your code will help you better understand computational concepts.\nGet familiar with reading error messages. When Python errors, it may give messages that are initially cryptic. Try the following.\n\nTake a deep breath.\nActually read the message—don’t just focus on what you think it might say. The error message will often include a meaningful name (e.g., SyntaxError or ZeroDivisionError), the line, and an arrow (marked by ^) to where the error occurred.\nThen, try to explain to yourself why this error occurred, before fixing the error. Many beginning programmers will want skip this step, opting to throw everything at the wall and then, once things are fixed, explain what happened. Instead, slow down and try to understand the problem first. Then the solution will be straightforward.\n\nTrace your code, but not at the expense of testing. Code tracing is the process of analyzing Python code, statement by statement and line by line, to understand program execution. This is a visual process and can sometimes be helped along by diagrams. Learning to trace code is learning procedural thinking—it is a key skill that helps break down exactly what your code is doing. Once you get the hang of tracing your code, you may be tempted to debug by just staring at your code—don’t do this! Instead, develop a healthy balance between developing an understanding of your code and testing if your code does what you expect.\n\n\nPractice with Errors\nTry these on for size:\nSyntax errors are errors in writing “valid” Python that cannot even create nonsensical Python code.\n\n3 ** / 4\n\n\n  Cell In[6], line 1\n    3 ** / 4\n         ^\nSyntaxError: invalid syntax\n\n\n\n\nWhy might the below code not error? (Hint: What does - represent?)\n\n9 ** - .5\n\n0.3333333333333333\n\n\nOnce you fix syntax errors, you may still encounter functionality errors, which can be errors caused during execution that leads to your program crashing. Here’s one common one:\n\n5 / 0\n\n\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 5 / 0\n\nZeroDivisionError: division by zero\n\n\n\n```",
    "crumbs": [
      "L01: Introduction",
      "Programming Basics"
    ]
  },
  {
    "objectID": "01-intro/programming-basics.html#summary",
    "href": "01-intro/programming-basics.html#summary",
    "title": "Programming Basics",
    "section": "Summary",
    "text": "Summary\nWe write code to tell our computer what to do.\n\nIn this class, and in many other settings, we use the Python programming language.\nWe write all of our code in Jupyter Notebooks, which allow us to see the output of our code in the same document in which we wrote our code. They’re commonly used in data science.\n\nBasic Python can be thought of as a calculator language, that takes expressions and computes their values.\n\nWe learned several different arithmetic operators, each of which can be used in an expression.\nPython stores integers and decimals in different ways.\n\nDebugging and good style make good programmers.\n\nComments help make our code more readable and sustainable.\nChoose names that are concise but descriptive.\nWhen our code has an error, the error message can help us fix it.\n\n\nCommon Python operators for numeric data types\n\n\n\n\n\n\n\n\nOperator\nSymbol\nExample Expression\nExpression Value\n\n\n\n\nAddition\n+\n2 + 3\n5\n\n\nSubtraction\n-\n15 - 4\n11\n\n\nMultiplication\n*\n-2 * 9\n-18\n\n\nDivision\n/\n15 / 2\n7.5\n\n\nInteger division\nCuts off remainder\n//\n15 // 2\n\n\nRemainder/Modulo\n%\n19 % 3\n1  (19 ÷ 3 = 6 Remainder 1)\n\n\nExponentiation\n**\n3 ** 2\n9",
    "crumbs": [
      "L01: Introduction",
      "Programming Basics"
    ]
  },
  {
    "objectID": "01-intro/programming-basics.html#external-reading",
    "href": "01-intro/programming-basics.html#external-reading",
    "title": "Programming Basics",
    "section": "External Reading",
    "text": "External Reading\n\n(mentioned in notes) Computational and Inferential Thinking, Ch 3.1, Ch 3.2, Ch 3.3\n(optional) Nick Parlante. Python Guide Python Math.",
    "crumbs": [
      "L01: Introduction",
      "Programming Basics"
    ]
  },
  {
    "objectID": "01-intro/deprecated-index.html",
    "href": "01-intro/deprecated-index.html",
    "title": "Course Introduction",
    "section": "",
    "text": "Consider this class as an application of computational thinking to data science. Naturally, Data Science can cover multiple contexts, and in this one we focus on social science contexts.\nThree pillars:\n\nComputer Science: Computational Thinking\nData Science: Exploratory data analysis.\nSocial Science: Computational Social Science\n\n(need to translate into learning objectives, but you get it)\nThis course can serve as a precursor to a statistics and inference course like Data 8 or as a standalone introduction to computational thinking for social scientists."
  },
  {
    "objectID": "01-intro/deprecated-index.html#what-will-i-learn-in-this-course",
    "href": "01-intro/deprecated-index.html#what-will-i-learn-in-this-course",
    "title": "Course Introduction",
    "section": "",
    "text": "Consider this class as an application of computational thinking to data science. Naturally, Data Science can cover multiple contexts, and in this one we focus on social science contexts.\nThree pillars:\n\nComputer Science: Computational Thinking\nData Science: Exploratory data analysis.\nSocial Science: Computational Social Science\n\n(need to translate into learning objectives, but you get it)\nThis course can serve as a precursor to a statistics and inference course like Data 8 or as a standalone introduction to computational thinking for social scientists."
  },
  {
    "objectID": "01-intro/deprecated-index.html#computational-thinking",
    "href": "01-intro/deprecated-index.html#computational-thinking",
    "title": "Course Introduction",
    "section": "Computational Thinking",
    "text": "Computational Thinking\n\nWhat is it?\n\nDefine computational thinking\n\nCommunications of the ACM, Jeanette Wing (PDF) and this follow-up article\n\nComputational Thinking is the thought processes involved in formulating problems and their solutions so that the solutions are represented in a form that can be effectively carried out by an information-processing agent (Cuny, Snyder, and Wing, 2010).\n\n\n\nAbstraction: A Core Concept\nWhat does this definition mean in the age of data?\n\nDefine abstraction\n\n\n“Abstraction gives us the power to scale and deal with complexity.” (Wing, 2010)"
  },
  {
    "objectID": "01-intro/deprecated-index.html#exploratory-data-analysis",
    "href": "01-intro/deprecated-index.html#exploratory-data-analysis",
    "title": "Course Introduction",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nSee Data 8 book: Chapter 1\n\nData Science is about drawing useful conclusions from large and diverse data sets through exploration, prediction, and inference. Exploration involves identifying patterns in information. Prediction involves using information we know to make informed guesses about values we wish we knew. Inference involves quantifying our degree of certainty: will the patterns that we found in our data also appear in new observations? How accurate are our predictions? Our primary tools for exploration are visualizations and descriptive statistics, for prediction are machine learning and optimization, and for inference are statistical tests and models.\n\nFrom Data 8 (1.1 Introduction):\n\nData are descriptions of the world around us, collected through observation and stored on computers. Computers enable us to infer properties of the world from these descriptions. Data science is the discipline of drawing conclusions from data using computation."
  },
  {
    "objectID": "01-intro/deprecated-index.html#computational-social-science",
    "href": "01-intro/deprecated-index.html#computational-social-science",
    "title": "Course Introduction",
    "section": "Computational Social Science",
    "text": "Computational Social Science\nWhat does it mean to study social phenomena?\nResearch methodologies: quantitative data, qualitative data\nMost methods contain a mix of both\nHow Define experiments\nIn the real world"
  },
  {
    "objectID": "01-intro/deprecated-index.html#external-reading",
    "href": "01-intro/deprecated-index.html#external-reading",
    "title": "Course Introduction",
    "section": "External Reading",
    "text": "External Reading\n\nComputational and Inferential Thinking, Ch 1.1"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About these Course Notes",
    "section": "",
    "text": "These course notes were compiled and created for Data 6: Introduction to Computational Thinking with Data Science and Society. This is an introductory, interdisciplinary course that focuses on the principles of computational thinking for the purposes exploratory data analysis and computational social science.",
    "crumbs": [
      "Home / About"
    ]
  },
  {
    "objectID": "index.html#disclaimer-is-this-a-textbook",
    "href": "index.html#disclaimer-is-this-a-textbook",
    "title": "About these Course Notes",
    "section": "Disclaimer: Is this a textbook?",
    "text": "Disclaimer: Is this a textbook?\nThese lecture notes are exactly that—lecture notes. That means that while they may follow the beats of class, they may not contain all the context needed to fully understand the course material and its theoretically underpinnings. This is a limitation of this curriculum being new and interdisciplinary. We source from many other foundational texts as needed; these textbook chapters are linked within the lecture notes themselves. You are expected to read these external links.",
    "crumbs": [
      "Home / About"
    ]
  },
  {
    "objectID": "index.html#course-links",
    "href": "index.html#course-links",
    "title": "About these Course Notes",
    "section": "Course Links",
    "text": "Course Links\ndata6.org: Syllabi and assignments for semesters at UC Berkeley.\nWe strongly recommend supplementing the notes presented here with the fantastic foundational texts prepared by UC Berkeley faculty instructors in Stat 20, Data 8, and CS 61A:\n\nData 8: Computational and Inferential Thinking: The Foundations of Data Science, 2nd Edition, by Ani Adhikari, John DeNero, David Wagner.\nCS 61A: Composing Programs, by John DeNero.\nStat 20: Course Notes by Andrew Bray.",
    "crumbs": [
      "Home / About"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "About these Course Notes",
    "section": "License",
    "text": "License\nThe contents of this work are licensed for free consumption under the following license: Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International",
    "crumbs": [
      "Home / About"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "About these Course Notes",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nMany faculty instructors and teaching assistants have contributed to the creation of Data 6. Special thanks to Suraj Rampure for one of the early iterations of this course.\nThis material is based upon work supported by the U.S. National Science Foundation under award Nos. 2245877, 2245878, 2245879, and the California Learning Lab. Please read more about our DUBOIS project activities here: https://dubois-ctds.github.io/",
    "crumbs": [
      "Home / About"
    ]
  },
  {
    "objectID": "05-variables/units-of-analysis.html",
    "href": "05-variables/units-of-analysis.html",
    "title": "Units of Analysis",
    "section": "",
    "text": "Operationalization also depends on our unit of analysis, or the level of social life about which we want to generalize. There are many different such units:\n\nIndividuals\nGroups (families, classes, gangs, …)\nLocalities (cities, counties, countries, …)\nOrganizations, industries, political units, social artifacts, etc.\n\nFor example, consider the concept “poverty.” From Carr et al.:\n\nCounting up the number of economic stressors that a person has faced in the past year (for example, trouble paying bills, getting behind on rent) would be appropriate for categorizing individual people but not neighborhoods. Neighborhood poverty is generally assessed through summary factors that can be more directly and comparably measured across large numbers of people, such as average income. One can try to identify the unit of analysis in a study by asking whether the researchers are trying to compare people to each other or neighborhoods to each other.",
    "crumbs": [
      "L05: Variables in Social Science",
      "Units of Analysis"
    ]
  },
  {
    "objectID": "05-variables/units-of-analysis.html#units-of-analysis",
    "href": "05-variables/units-of-analysis.html#units-of-analysis",
    "title": "Units of Analysis",
    "section": "",
    "text": "Operationalization also depends on our unit of analysis, or the level of social life about which we want to generalize. There are many different such units:\n\nIndividuals\nGroups (families, classes, gangs, …)\nLocalities (cities, counties, countries, …)\nOrganizations, industries, political units, social artifacts, etc.\n\nFor example, consider the concept “poverty.” From Carr et al.:\n\nCounting up the number of economic stressors that a person has faced in the past year (for example, trouble paying bills, getting behind on rent) would be appropriate for categorizing individual people but not neighborhoods. Neighborhood poverty is generally assessed through summary factors that can be more directly and comparably measured across large numbers of people, such as average income. One can try to identify the unit of analysis in a study by asking whether the researchers are trying to compare people to each other or neighborhoods to each other.",
    "crumbs": [
      "L05: Variables in Social Science",
      "Units of Analysis"
    ]
  },
  {
    "objectID": "05-variables/units-of-analysis.html#aggregation-and-disaggregation",
    "href": "05-variables/units-of-analysis.html#aggregation-and-disaggregation",
    "title": "Units of Analysis",
    "section": "Aggregation and Disaggregation",
    "text": "Aggregation and Disaggregation\nIt is possible to operationalize variables at larger units of analysis using variables at smaller units of analysis via a process called aggregation.\nAggregation - “Roll up” a variable measured on a fine-grained unit of analysis (e.g., individuals) into a variable on a coarser-grained unit of analysis (e.g., groups).\n\nIncome of many individuals in geographic regions → average income by region\nUsually done through counting or averaging.\n\nFrom Carr et al.:\n\nA discussion of historical change in the onset of puberty in the United States occurs at a national level of analysis. Yet, researchers measure that national level by getting data on puberty from individuals and then calculating an average across the group. A country’s average age of puberty is a national-level concept, and its measurement is based on the aggregation of individual-level data.\n\nWe will soon see that it can often be challenging to move in the other direction with disaggregation:\nDisaggregation: “Drill down” a variable measured on a coarser-grained unit of analysis (e.g., region) into a variable on a coarser-grained unit of analysis (e.g., groups within that region)\n\nGenerally performed to identify confounding (or mediating) variables to disentangle the impact of certain variables (more later)\nAverage income by geographic region → average income by race/ethnicity by region",
    "crumbs": [
      "L05: Variables in Social Science",
      "Units of Analysis"
    ]
  },
  {
    "objectID": "05-variables/units-of-analysis.html#example-dataset-american-community-survey",
    "href": "05-variables/units-of-analysis.html#example-dataset-american-community-survey",
    "title": "Units of Analysis",
    "section": "Example Dataset: American Community Survey",
    "text": "Example Dataset: American Community Survey\nLet’s return to our American Community Survey (ACS) 2020 data. It shows education levels of adults 25 years or higher by state.\n\nACS 2020.\n\n\n\n\n\n\n\n\nState\nEstimated total state population\nEstimated high school graduate or higher (%)\nEstimated bachelor’s degree or higher (%)\n\n\n\n\nAlabama\n3,344,006\n86.9\n26.2\n\n\nCalifornia\n26,665,143\n83.9\n34.7\n\n\nFlorida\n15,255,326\n88.5\n30.5\n\n\nNew York\n13,649,157\n87.2\n37.5\n\n\nTexas\n18,449,851\n84.4\n30.7\n\n\n\nFrom the ACS webpage, the American Community Survey (ACS) is an ongoing monthly survey that collects detailed housing and socioeconomic data.\n\n\n\n\n\n\n\nFigure 1: ACS Household survey, which collects data on individual households.\n\n\n\n\nThere are (at least) two datasets collected by the ACS: A private dataset of survey responses by household (Figure 1), and a public-facing dataset of responses by geographic region. The variables for the geographic region, a larger unit of analysis, are constructed via aggregation and estimation (Figure 2):\n\n\n\n\n\n\n\nFigure 2: ACS reported public data, which reports aggregated data of households across a geographic region.\n\n\n\n\nSimple forms of aggregation are straightforward and involve counting and averaging—methods that are very possible using our limited Data Science toolkit thus far. However, disaggregation cannot be done without individual datapoints! There are various methods of estimating individuals from averages using statistics and distributions; we discuss this briefly in a few weeks, but you can take a statistics course for more information.",
    "crumbs": [
      "L05: Variables in Social Science",
      "Units of Analysis"
    ]
  },
  {
    "objectID": "05-variables/units-of-analysis.html#privacy-and-pums",
    "href": "05-variables/units-of-analysis.html#privacy-and-pums",
    "title": "Units of Analysis",
    "section": "Privacy and PUMS",
    "text": "Privacy and PUMS\nWhy would the ACS choose to release aggregate data publicly, but keep individual data private? Releasing “fine-grained” data about individuals is a privacy issue. It puts individuals in that dataset at risk of being identified beyond the research. In particular, small, already vulnerable populations are often more easily identified.\nNevertheless, the researchers at ACS understand the value of their government-collected dataset for supporting social science researchers in answering questions about large units of analysis. ACS therefore provides aggregated views of data by region and specific disaggregations by certain demographic factors, such as income and sex.\nResearchers who may be studying questions that try to disentangle the impact of demographic factors that ACS disaggregated data does not support (e.g., race/ethnicity) may choose to connect the ACS dataset with a second dataset that has finer-grained data on an individual level, then build up aggregate measures based on differently defined units of analysis.\nThe ACS Public Use Microdata Sample (PUMS) is one such source. PUMS is a smaller sample of records from individual people and/or housing units that uses a combination of techniques (including differential privacy and synthetic data) to preserve individual privacy (source1, source2). We hope that later in this class we can discuss this privacy protection process.",
    "crumbs": [
      "L05: Variables in Social Science",
      "Units of Analysis"
    ]
  },
  {
    "objectID": "05-variables/units-of-analysis.html#external-reading",
    "href": "05-variables/units-of-analysis.html#external-reading",
    "title": "Units of Analysis",
    "section": "External Reading",
    "text": "External Reading\n“Chapter 4: From Concepts to Models.” Elizabeth Heger Boyle, Deborah Carr, Benjamin Cornwell, Shelley Correll, Robert Crosnoe, Jeremy Freese, and Waters, Mary C. 2017. The Art and Science of Social Research. New York: W. W. Norton & Company.",
    "crumbs": [
      "L05: Variables in Social Science",
      "Units of Analysis"
    ]
  },
  {
    "objectID": "05-variables/units-of-analysis.html#references",
    "href": "05-variables/units-of-analysis.html#references",
    "title": "Units of Analysis",
    "section": "References",
    "text": "References\nU.S. Census Bureau, “EDUCATIONAL ATTAINMENT,” American Community Survey 5-Year Estimates Subject Tables, Table S1501, 2020, https://data.census.gov/table/ACSST5Y2020.S1501?q=2020+education&t=Age+and+Sex:Educational+Attainment&g=010XX00US$0400000, accessed on August 24, 2025.\nU.S. Census Bureau, “Design and Methodology Report.” https://www.census.gov/programs-surveys/acs/methodology/design-and-methodology.html, accessed on September 2, 2025.\nU.S. Census Bureau, “Public Use Microdata Sample (PUMS).” https://www.census.gov/programs-surveys/acs/microdata.html, accessed on September 2, 2025.",
    "crumbs": [
      "L05: Variables in Social Science",
      "Units of Analysis"
    ]
  },
  {
    "objectID": "reference.html",
    "href": "reference.html",
    "title": "🐍 Data 6 Python Reference",
    "section": "",
    "text": "If you’re new to reading documentation, looking at this page might feel a little overwhelming, but don’t worry — the point of this class is not to memorize all of these functions or their arguments. For exams and quizzes, you will be provided with a reference sheet that contains all of the functions you may need on the exam.\nHowever, we do expect you to understand how to use this Python Reference to understand new functions, and to help with debugging when things go wrong. Learning how to read and understand documentation is a key to becoming a good data scientist. In fact, even course staff continue to use the Python Reference to refresh their memory about certain functions.\nOf course, the Python Reference can only provide information about the basics of the functions you’ll use in Data 6. The best knowledge about functions comes from using these functions in code you write in labs or homeworks. If you get stuck when using a certain function, we encourage you to come to office hours or ask a question on Ed.\n\n\n\ndef my_function(num):\n    return num ** 3\n\nIn the function above, my_function is the name of the function, which takes one argument called num. The data type of the input is an int or float, and the function returns the number raised to the power 3, which is also an int or float.\n\n\n\n\nThe Function column tells you how to call the function and what arguments it accepts. Everything written in this font is code or refers to a particular argument in the function (e.g. num in np.sqrt(num)).\nThe Description column gives you a brief description of what the function does, including what each argument is used for\nThe Input column tells you what data type each argument needs to be. If you’re getting a TypeError, it might be because your inputs are of the wrong type. Data types are indicated in bold (e.g. string or Table).\nThe Output column tells you what the function returns and what data type it is.",
    "crumbs": [
      "Data 6 Python Reference"
    ]
  },
  {
    "objectID": "reference.html#understanding-this-documentation-page",
    "href": "reference.html#understanding-this-documentation-page",
    "title": "🐍 Data 6 Python Reference",
    "section": "",
    "text": "If you’re new to reading documentation, looking at this page might feel a little overwhelming, but don’t worry — the point of this class is not to memorize all of these functions or their arguments. For exams and quizzes, you will be provided with a reference sheet that contains all of the functions you may need on the exam.\nHowever, we do expect you to understand how to use this Python Reference to understand new functions, and to help with debugging when things go wrong. Learning how to read and understand documentation is a key to becoming a good data scientist. In fact, even course staff continue to use the Python Reference to refresh their memory about certain functions.\nOf course, the Python Reference can only provide information about the basics of the functions you’ll use in Data 6. The best knowledge about functions comes from using these functions in code you write in labs or homeworks. If you get stuck when using a certain function, we encourage you to come to office hours or ask a question on Ed.\n\n\n\ndef my_function(num):\n    return num ** 3\n\nIn the function above, my_function is the name of the function, which takes one argument called num. The data type of the input is an int or float, and the function returns the number raised to the power 3, which is also an int or float.\n\n\n\n\nThe Function column tells you how to call the function and what arguments it accepts. Everything written in this font is code or refers to a particular argument in the function (e.g. num in np.sqrt(num)).\nThe Description column gives you a brief description of what the function does, including what each argument is used for\nThe Input column tells you what data type each argument needs to be. If you’re getting a TypeError, it might be because your inputs are of the wrong type. Data types are indicated in bold (e.g. string or Table).\nThe Output column tells you what the function returns and what data type it is.",
    "crumbs": [
      "Data 6 Python Reference"
    ]
  },
  {
    "objectID": "reference.html#built-in-python-functions",
    "href": "reference.html#built-in-python-functions",
    "title": "🐍 Data 6 Python Reference",
    "section": "Built-In Python Functions",
    "text": "Built-In Python Functions\n\n\n\n\n\n\n\n\n\nFunction\nDescription\nInput\nOutput\n\n\n\n\nstr(val)\nConverts val to a string\nA value of any type (int, float, NoneType, etc.)\nThe value as a string\n\n\nint(num)\nConverts num to an int\nA numerical value (represented as a string or float)\nThe value as an int\n\n\nfloat(num)\nConverts num to a float\nA numerical value (represented as a string or int)\nThe value as a float\n\n\nlen(arr)\nReturns the length of arr\narray or list\nint: the length of the array or list\n\n\nmax(arr)\nReturns the maximum value in arr\narray or list\nThe maximum value the array (usually an int)\n\n\nmin(arr)\nReturns the minimum value in arr\narray or list\nThe minimum value the array (usually an int)\n\n\nsum(arr)\nReturns the sum of the values in arr\narray or list\nint or float: the sum of the values in the array\n\n\nabs(num)\nReturns the absolute value of num\nint or float\nint or float\n\n\nprint(input, ...)\nPrints the input. Multiple inputs can be passed, and they will be separated by spaces by default.\ninput: any inputs to print \nNone\n\n\ntype(object)\nReturns the type of object.\nobject: the object whose type is to be determined\ntype: the type of the object",
    "crumbs": [
      "Data 6 Python Reference"
    ]
  },
  {
    "objectID": "reference.html#numpy-array-functions",
    "href": "reference.html#numpy-array-functions",
    "title": "🐍 Data 6 Python Reference",
    "section": "NumPy Array Functions",
    "text": "NumPy Array Functions\n\n\n\n\n\n\n\n\n\nFunction\nDescription\nInput\nOutput\n\n\n\n\nmake_array(val1, val2, ...)\nMakes a NumPy array with the inputted values\nA sequence of values\nAn array with those values\n\n\nnp.mean(arr) or np.average(arr)\nCalculates the average value of arr\nAn array of numbers\nfloat: The average of the array\n\n\nnp.sum(arr)\nReturns the sum of the values in arr\narray\nint or float: the sum of the values in the array\n\n\nnp.prod(arr)\nReturns the product of the values in arr\narray\nint or float: the product of the values in the array\n\n\nnp.sqrt(num)\nCalculates the square root of num\nint or float\nfloat : the square root of the number\n\n\nnp.arange(stop), np.arange(start, stop), or np.arange(start, stop, step)\nCreates an array of sequential numbers starting at start, going up in increments of step, and going up to but excluding stop. Default start is 0, default step is 1\nint or float\narray\n\n\nnp.count_nonzero(arr)\nReturns the number of non-zero (or True) elements in an array\nAn array of values\nint: the number of non-zero values in arr\n\n\nnp.append(arr, item)\nAppends item to the end of arr. Does not modify the original array.\n1. array to append to  2. item to append (any type)\narray: a new array with the appended item\n\n\nnp.cumsum(arr)\nReturns the cumulative sum of the elements in arr, where each element is the sum of all preceding elements including itself\narray\narray: the cumulative sum of the values in the array\n\n\nnp.diff(arr)\nComputes the difference between consecutive elements in arr.\narray\narray: the differences between consecutive elements in the array containing len(arr) - 1 elements",
    "crumbs": [
      "Data 6 Python Reference"
    ]
  },
  {
    "objectID": "reference.html#string-methods",
    "href": "reference.html#string-methods",
    "title": "🐍 Data 6 Python Reference",
    "section": "String Methods",
    "text": "String Methods\n\n\n\n\n\n\n\n\n\nFunction\nDescription\nInput\nOutput\n\n\n\n\nstr.split(separator, maxsplit)\nSplits str into a list of substrings using the specified separator. If separator is not provided, splits at any whitespace. You can also use the optional argument maxsplit to limit the number of splits.\n1. (Optional) separator: the delimiter used to split str  2. (Optional) maxsplit: maximum number of splits\nlist of substrings\n\n\nstr.join(iterable)\nConcatenates the elements in iterable (usually a list or array) into a single string, with each element separated by str.\niterable: an iterable of strings to join (can be an array or list of strings)\nstring: a single string formed by joining the elements of iterable with the separator str\n\n\nstr.replace(old, new)\nReturns a copy of the string with all occurrences of the substring old replaced by new.\nold: the substring to be replaced.  new: the substring to replace old with.\nstring: a new string where occurrences of old have been replaced by new.",
    "crumbs": [
      "Data 6 Python Reference"
    ]
  },
  {
    "objectID": "reference.html#tables-and-table-methods",
    "href": "reference.html#tables-and-table-methods",
    "title": "🐍 Data 6 Python Reference",
    "section": "Tables and Table Methods",
    "text": "Tables and Table Methods\n\n\n\nFunction\nDescription\nInput\nOutput\n\n\n\n\nTable()\nCreates an empty table, usually to extend with data\nNone\nAn empty Table\n\n\nTable().read_table(filename)\nCreate a table from a data file\nstring: the name of the file\n\n\n\ntbl.with_column(name, values) or tbl.with_columns(n1, v1, n2, v2, ...)\nAdds an extra column onto tbl with the label name and values as the column values\n1. string: name of the new column  2. array: values in the column\nTable: a copy of the original table with the new column(s)\n\n\ntbl.column(col)\nReturns the values in a column in tbl\nstring or int: the column name or index\narray: the values in that column\n\n\ntbl.num_rows\nCompute the number of rows in tbl\nNone\nint: the number of rows in the table\n\n\ntbl.num_columns\nCompute the number of columns in tbl\nNone\nint: the number of columns in the table\n\n\ntbl.labels\nReturns the labels in tbl\nNone\narray: the names of each column as strings\n\n\ntbl.select(col1, col2, ...)\nCreates a copy of tbl only with the selected columns\nstring or int: the column name(s) or index(es) to be included in the table\nTable with the selected columns\n\n\ntbl.drop(col1, col2, ...)\nCreates a copy of tbl without the selected columns\nstring or int: the column name(s) or index(es) to be dropped from the table\nTable without the selected columns\n\n\ntbl.relabeled(old_label, new_label)\nCreates a new table, changing the column name specified by old_label to new_label, and leaves the original table unchanged.\n1. string: the old column name  2. string the new column name\nTable: a copy of the original table with the changed column name\n\n\ntbl.show(n)\nDisplays the first n rows of tbl. If no argument is specified, the function defaults to showing the entire table\n(Optional) int: number of rows to be displayed\nNone (table is displayed)\n\n\ntbl.sort(column_name)\nSorts the rows of tbl by the values in the column_name column. Defaults to ascending order unless the optional argument descending=True is included.\n1. string or int: name or index of the column to sort  2. (Optional) descending=True\nTable: a copy of the original table with the column sorted\n\n\ntbl.where(column, predicate)\nCreates a copy of tbl containing only the rows where the value of column matches the predicate. See Table.where predicates below.\n1. string or int: column name or index  2. are.(...) predicate\nTable: a copy of the original table with only the rows that match the predicate\n\n\ntbl.take(row_indices)\nCreates a table with only the rows at the given indices. row_indices is either an array of indices or an integer corresponding to one index.\nint or array: indices of rows to be included in the table\nTable: a copy of the original table with only the rows at the given indices\n\n\ntbl.apply(function) or tbl.apply(function, col1, col2, ...)\nReturns an array of values resulting from applying a function to each item in a column.\n1. Function: function to apply to column  2. (Optional) string or int: the column name(s) or index(es) to apply the function to\narray containing an element for each value in the original column after applying the function to it\n\n\ntbl.group(column_or_columns, function)\nGroups rows in tbl by unique values or combinations of values in a column(s). Multiple columns must be entered as an array of strings. Values in the other columns are aggregated by count (by default) or the optional argument function. You can visualize the group function here.\n1. string or array of strings: column(s) on which to group  2. (Optional) Function: function to aggregate values in cells (defaults to counting rows)\nTable a new groupped table\n\n\ntbl.pivot(col1, col2) or tbl.pivot(col1, col2, values, collect)\nCreates a pivot table where each unique value in col1 has its own column and each unique value in col2 has its own row. Counts or aggregates values from a third column, collected with some function. If the values and collect arguments are not included, pivot defaults to returning counts in the cells. You can visualize the pivot function here.\n1. string: name of the column in tbl whose unique values will make up the columns of the pivot table  2. string: name of column in tbl whose unique values will make up the rows of the pivot table  3. (Optional) string: name of the column in tbl that describes the values of cells in the pivot table  4. (Optional) Function: how the values are collected (e.g. sum or np.mean)\nTable: a new pivot table\n\n\ntblA.join(colA, tblB) or tblA.join(colA, tblB, colB)\nGenerate a table with the columns of tblA and tblB, containing rows for all values in colA and colB that appear in tblA and tblB, respectively. By default, colB is the same value as colA. colA and colB must be strings specifying column names.\n1. string: name of column in tblA with values to join on  2. Table: the other table  3. (Optional) string: the name of the shared column in tblB, if column names are different between the tables\nTable: a new combined table\n\n\ntbl.with_row(values)\nAdds a new row with the specified values to tbl\n1. list or array: values to add as a new row\nTable: a copy of the original table with the new row\n\n\ntbl.with_rows(list_of_rows)\nAdds multiple rows to tbl using a list of rows\n1. list of lists or arrays: each list/array represents a new row\nTable: a copy of the original table with the new rows",
    "crumbs": [
      "Data 6 Python Reference"
    ]
  },
  {
    "objectID": "reference.html#visualization-functions",
    "href": "reference.html#visualization-functions",
    "title": "🐍 Data 6 Python Reference",
    "section": "Visualization Functions",
    "text": "Visualization Functions\n\n\n\n\n\n\n\n\n\nFunction\nDescription\nInput\nOutput\n\n\n\n\ntbl.barh(categories) or tbl.barh(categories, values)\nDisplays a horizontal bar chart with bars for each category in the column categories. values specifies the column corresponding to the size of each bar, but is unnecessary if the table only has two columns. Optional argument overlay (default is True) specifies whether grouped bar charts should be overlaid or on separate plots.\n1. string: name of the column with categories  2. (Optional) string: name of the column with values corresponding to the categories\nNone: draws a bar chart\n\n\ntbl.hist(column)\nGenerates a histogram of the numerical values in column. Optional arguments group (to specify categorical column to group on), bins (to specify custom bins), and overlay to specify overlaid or separate histograms.\nstring: name of the column\nNone: draws a histogram\n\n\ntbl.plot(x_column, y_column) or tbl.plot(x_column)\nDraws a line plot consisting of one point for each row in tbl. If only x_column is specified, plot will plot the rest of the columns on the y-axis with different colored lines. Optional argument overlay (default is True) specifies whether multiple lines should be overlaid or on separate plots.\n1. string: name of the column on the x-axis  2. string: name of the column on the y-axis\nNone: draws a line graph\n\n\ntbl.scatter(x_column, y_column)\nDraws a scatter plot consisting of one point for each row in tbl. The optional argument fit_line=True can be included to draw a line of best fit through the scatter plot. The optional arguments group (to specify categorical column to group on) and sizes (to specify a numerical column for bubble sizes) can also be used to encode additional variables.\n1. string: name of the column on the x-axis  2. string: name of the column on the y-axis  3. (Optional) fit_line=True\nNone: draws a scatter plot",
    "crumbs": [
      "Data 6 Python Reference"
    ]
  },
  {
    "objectID": "reference.html#table.where-predicates",
    "href": "reference.html#table.where-predicates",
    "title": "🐍 Data 6 Python Reference",
    "section": "Table.where Predicates",
    "text": "Table.where Predicates\nThese functions can be passed in as the second argument to tbl.where(..) and act as a condition by which to select rows from tbl.\n\n\n\n\n\n\n\nPredicate\nDescription\n\n\n\n\nare.equal_to(Z)\nEqual to Z (can be an int, float or string)\n\n\nare.not_equal_to(Z)\nNot equal to ‘Z’ can be a number (int or float) or a string)\n\n\nare.above(x)\nGreater than x\n\n\nare.above_or_equal_to(x)\nGreater than or equal to x\n\n\nare.below(x)\nLess than x\n\n\nare.below_or_equal_to(x)\nLess than or equal to x\n\n\nare.between(x,y)\nGreater than or equal to x and less than y\n\n\nare.between_or_equal_to(x,y)\nGreater than or equal to x, and less than or equal to y\n\n\nare.strictly_between(x,y)\nGreater than x and less than y\n\n\nare.contained_in(A)\nTrue if it is a substring of A (if A is a string) or an element of A (if A is an array)\n\n\nare.containing(S)\nContains the string S",
    "crumbs": [
      "Data 6 Python Reference"
    ]
  },
  {
    "objectID": "reference.html#more-documentation",
    "href": "reference.html#more-documentation",
    "title": "🐍 Data 6 Python Reference",
    "section": "More Documentation",
    "text": "More Documentation\nThe Data 6 Python reference guide is based on the Data 8 Python Reference. More detailed Python documentation is available here.",
    "crumbs": [
      "Data 6 Python Reference"
    ]
  },
  {
    "objectID": "05-variables/index.html",
    "href": "05-variables/index.html",
    "title": "Variables and Variable Types",
    "section": "",
    "text": "In data science, a variable is a measurable feature, attribute, and/or representation of a concept. It can have different values for different individuals.\nVariables are often empirical measurements; they are metrics that researchers create to approximate the specific dimensions of an abstract concept of a research question.\nThe process of going from a concept, which may often be unmeasurable, to a definition of variable(s) and methods of measurement of these variable(s) is a process broadly referred to as operationalization.\n\nExamples of operationalizing concepts into variables\n\n\n\n\n\n\nConcept\nVariable(s)\n\n\n\n\nEducation level\nHighest degree earned\n\n\nFamily background\nHousehold income, in thousands of dollarsHousehold sizeParental/guardian immigrant status\n\n\n\nFrom the above it should be evident that the operationalization process has pitfalls! Wikipedia describes this issue well with an example operationalizing the concept “anger.” One operation could be directly asking each participant their anger level. However, this self-evaluation process is not only subjective (individuals may define their own anger levels differently) but also often impractical (e.g., surveys or interviews may be impossible in an experimental setting).\nIn summary, operationalization necessarily highlights certain dimensions of a concept, constructing certain precise dimensions and measurements while hiding or obscuring others. However, operationalization makes research practical. Through operational definitions, it constructs procedures for measuring and collecting data. By translating immeasurable concepts into numeric or categorical quantities, one can then conduct statistical and graphical analysis on the data.",
    "crumbs": [
      "L05: Variables in Social Science"
    ]
  },
  {
    "objectID": "05-variables/index.html#variables-operationalization-of-concepts",
    "href": "05-variables/index.html#variables-operationalization-of-concepts",
    "title": "Variables and Variable Types",
    "section": "",
    "text": "In data science, a variable is a measurable feature, attribute, and/or representation of a concept. It can have different values for different individuals.\nVariables are often empirical measurements; they are metrics that researchers create to approximate the specific dimensions of an abstract concept of a research question.\nThe process of going from a concept, which may often be unmeasurable, to a definition of variable(s) and methods of measurement of these variable(s) is a process broadly referred to as operationalization.\n\nExamples of operationalizing concepts into variables\n\n\n\n\n\n\nConcept\nVariable(s)\n\n\n\n\nEducation level\nHighest degree earned\n\n\nFamily background\nHousehold income, in thousands of dollarsHousehold sizeParental/guardian immigrant status\n\n\n\nFrom the above it should be evident that the operationalization process has pitfalls! Wikipedia describes this issue well with an example operationalizing the concept “anger.” One operation could be directly asking each participant their anger level. However, this self-evaluation process is not only subjective (individuals may define their own anger levels differently) but also often impractical (e.g., surveys or interviews may be impossible in an experimental setting).\nIn summary, operationalization necessarily highlights certain dimensions of a concept, constructing certain precise dimensions and measurements while hiding or obscuring others. However, operationalization makes research practical. Through operational definitions, it constructs procedures for measuring and collecting data. By translating immeasurable concepts into numeric or categorical quantities, one can then conduct statistical and graphical analysis on the data.",
    "crumbs": [
      "L05: Variables in Social Science"
    ]
  },
  {
    "objectID": "05-variables/index.html#variables-in-tabular-data",
    "href": "05-variables/index.html#variables-in-tabular-data",
    "title": "Variables and Variable Types",
    "section": "Variables in Tabular Data",
    "text": "Variables in Tabular Data\nWe will see that as data scientists, we may often start with a dataset involving data that have already been collected for us. These datasets may have been constructed to answer specific questions (we’ll get into the research methods a bit later) and later shared to the broader public for transparency and reproducibility.\nIt is challenging to use another person’s data! The concepts have already been operationalized into variables; or the quality of data may vary widely; or multiple data sources may be needed to construct a reasonable first step towards a new research question. We’ll see more benefits and drawbacks in our examples this semester.\nFor now, we focus on variables as they exist in tabular data. In most of the tabular datasets we will examine, variables correspond to columns of features. Each row is a record of a datapoint, with different values of variables measured for that datapoint.\n\n\n\n\n\n\n\nFigure 1: Variables as columns.",
    "crumbs": [
      "L05: Variables in Social Science"
    ]
  },
  {
    "objectID": "05-variables/index.html#variable-types",
    "href": "05-variables/index.html#variable-types",
    "title": "Variables and Variable Types",
    "section": "Variable Types",
    "text": "Variable Types\nVariable types are different from Python data types like string, int, etc. Variable types help us make more informed decisions about how to measure and analyze variables: to make comparisons, create visualizations, and draw conclusions.\n\nNumerical/Quantitative Variable: a variable that takes numbers as values and where the magnitude of the number has a quantitative meaning.\n\nDiscrete: A numerical variable that takes values that have jumps between them.\nContinuous: A numerical variable that takes values on an interval of the real number line.\n\nCategorical/Qualitative Variable: a variable that takes categories as values. Each unique category is called a level.\n\nOrdinal: A categorical variable with levels that have a natural ordering.\nNominal: A categorical variable with levels that have no ordering.\n\n\nFigure 2 has examples of each variable type.\n\n\n\n\n\n\n\nFigure 2: Variable Types.\n\n\n\n\nWhat do we mean by “meaningful” arithmetic? From Stat 20:\n\nWhat unites both types of numerical variables is that the magnitude of the numbers have meaning and you can perform mathematical operations on them and the result also has meaning. It is possible and meaningful to talk about the average air temperature across three locations. It is also possible and meaningful to talk about the sum total number of people across ten households.\n\nJust because a variable has numbers for values does not make it numerical. Consider phone area codes: Berkeley is 510, San Francisco is 415, Palo Alto is 650, and so on. While area codes are numbers, you can’t do any arithmetic—comparative or otherwise—with them that “makes sense”: what does it mean to add area codes? have one area code (650) that is “larger” than another (510)? Area codes are therefore an example of a nominal categorical variable.\nVariable types are closely tied to measurement. Consider the following survey item from Stat 20, which describes a common ordinal categorical variable in opinion surveys:\n\nConsider the question:“Do you strongly agree, agree, feel neutral about, disagree, or strongly disagree with the following statement: Dogs are better than cats?” When you record answers to this question, you’re recording measurements on a categorical variable that takes values “strongly agree”, “agree”, “neutral”, “disagree”, “strongly disagree”. Those are the levels of the categorical variable and they have a natural ordering: “strongly agree” is closer to “agree” than it is to “strongly disagree”.\n\nThe above is an example of a Likert item. The researcher likely wanted a way to measure respondents’ sentiment of dogs and cats—an abstract concept. To do so, the researcher distributed a survey to respondents that operationalized this concept as a 5-point ordinal categorical variable. (There are of course other ways to measure this sentiment, such as by interviewing respondents and asking the open-ended question, “Dogs or cats?” but these yield a different set of possible responses and, consequently a different set of variables.)\nAside: What is a discrete variable type, really? If you think about it deeply from a Computer Science perspective, there will be cases in which continuous numerical variables may seem discrete: after all, price (in U.S. dollars) is counted to the cent, and so there are a discrete number of prices for, say, a pound of apples (we hope). However, in the data world we consider how the variable type may inform how we construct informative visualizations of the variable itself and compare it to other values. In short, variable typing informs our choice of using histograms vs. scatter plots, and so on. More later on this.",
    "crumbs": [
      "L05: Variables in Social Science"
    ]
  },
  {
    "objectID": "05-variables/index.html#external-reading",
    "href": "05-variables/index.html#external-reading",
    "title": "Variables and Variable Types",
    "section": "External Reading",
    "text": "External Reading\n\n(mentioned in notes) “Chapter 4: From Concepts to Models.” Elizabeth Heger Boyle, Deborah Carr, Benjamin Cornwell, Shelley Correll, Robert Crosnoe, Jeremy Freese, and Waters, Mary C. 2017. The Art and Science of Social Research. New York: W. W. Norton & Company.\n(mentioned in notes) Stat 20 notes, Taxonomy of Data",
    "crumbs": [
      "L05: Variables in Social Science"
    ]
  },
  {
    "objectID": "05-variables/index.html#references",
    "href": "05-variables/index.html#references",
    "title": "Variables and Variable Types",
    "section": "References",
    "text": "References\nU.S. Census Bureau, “EDUCATIONAL ATTAINMENT,” American Community Survey 5-Year Estimates Subject Tables, Table S1501, 2020, https://data.census.gov/table/ACSST5Y2020.S1501?q=2020+education&t=Age+and+Sex:Educational+Attainment&g=010XX00US$0400000, accessed on August 24, 2025.\nU.S. Census Bureau, “Design and Methodology Report.” https://www.census.gov/programs-surveys/acs/methodology/design-and-methodology.html, accessed on September 2, 2025.\nU.S. Census Bureau, “Public Use Microdata Sample (PUMS).” https://www.census.gov/programs-surveys/acs/microdata.html, accessed on September 2, 2025.",
    "crumbs": [
      "L05: Variables in Social Science"
    ]
  },
  {
    "objectID": "01-intro/jupyter-notebook.html",
    "href": "01-intro/jupyter-notebook.html",
    "title": "Jupyter Notebooks",
    "section": "",
    "text": "This course uses the Python 3 programming language in the Jupyter Notebook environment. By the end of this note, you’ll understand what that means.",
    "crumbs": [
      "L01: Introduction",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "01-intro/jupyter-notebook.html#python-3",
    "href": "01-intro/jupyter-notebook.html#python-3",
    "title": "Jupyter Notebooks",
    "section": "Python 3",
    "text": "Python 3\nFrom Wikipedia:\n\nA computer program is a [set or sequence] of instructions in a programming language.\n\nCode refers to a computer program written in a particular programming language. In this class, we use the Python 3 programming language. It is powerful and widely used in many computing applications, from web development, scripting, and scientific computing to data science and machine learning. It’s also extremely popular worldwide (Statista, 2025).\nComputer programs are nothing more than recipes: we write programs that tell the computer exactly what to do, and it does exactly that—nothing more, and nothing less.\nYou may be wondering—how can computers be simultaneously so powerful and so primitive? Why does everything in today’s age involve computers or computational technology? In part, this “age of computing” can considered as a complex system of advanced programs run on powerful computing machines to transform complex data. All three of these components involve human scientists and engineers: to write and design the programs, to collect and structure the data, and to design and build the computing machines.",
    "crumbs": [
      "L01: Introduction",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "01-intro/jupyter-notebook.html#jupyter-notebooks",
    "href": "01-intro/jupyter-notebook.html#jupyter-notebooks",
    "title": "Jupyter Notebooks",
    "section": "Jupyter Notebooks",
    "text": "Jupyter Notebooks\nIn order to run computer programs, we need a way to execute code written in a programming language on a computer. Development, also known as process of designing, iterating, and testing computer programs, often takes place in an environment which can support all of these tasks.\nThe environment we will use is Jupyter Notebook, which allows us to write and run code within a single .ipynb document (i.e., notebook). They also allow us to embedded text and code.\n\n\n\n\n\n\n\nFigure 1: An example of a Jupyter Notebook.\n\n\n\n\nThere’s a lot going on in the above Jupyter Notebook screenshot: there is code, there is output from running code, there are pictures, and there is (non-code) text. We’ll get to understanding all of these components in due time.\nBut this screenshot also elucidates why a tool like Jupyter Notebook is so important to doing data science work. Data Science often requires the use of computation and visualizations and the production of written reports. Notebooks support all three of these, in the same document.\n\n\n\n\n\n\nAside\n\n\n\n\n\nThe Project Jupyter community actually started at UC Berkeley. Professor Fernando Perez of Statistics created an interactive Python environment as part of his graduate studies in Physics, and the rest is history.\n\n\n\n\n\n\n\n\n\nAside 2\n\n\n\n\n\nAside 2: Jupyter can run things other than Python—in fact, Jupyter’s namesake is the three core languages it supports: Julia, Python, and R.\n\n\n\nIf you take more Computer Science and Data Science classes, you will learn about more tools for programming and statistics. In this class we will focus on using Jupyter Notebooks to develop Python code.",
    "crumbs": [
      "L01: Introduction",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "01-intro/jupyter-notebook.html#datahub",
    "href": "01-intro/jupyter-notebook.html#datahub",
    "title": "Jupyter Notebooks",
    "section": "DataHub",
    "text": "DataHub\nDataHub is the web-based environment we will use in this course for developing and running Jupyter Notebooks. Some features:\n\nDataHub is a Berkeley-hosted server that runs Jupyter notebooks.\nAll students have their own DataHub “container”; think of this as your own virtual computer.\nThis is where you will work on all assignments.\nYou will not need to install anything locally (meaning that you could theoretically do all assignments for this class on your phone, but we recommend giving your fingers and your eyes a break). All you need is a web browser.\nCourse staff can access everything in your DataHub to help debug your code.\n\n\n\n\n\n\n\nAccess DataHub\n\n\n\nIn this class, there are two common ways to develop Jupyter Notebooks:\n\nGo to http://datahub.berkeley.edu. Make a new notebook, or open an existing one.\nFrom our course website, often by clicking on code links or assignment links. These will often create a copy of a notebook skeleton, which you can then run or edit.\n\n\n\nGenerally, we will not be creating notebooks from scratch. Instead, the course staff have helped write scaffolding code and instructions for activities that are designed to help you understand the fundamentals.",
    "crumbs": [
      "L01: Introduction",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "01-intro/jupyter-notebook.html#jupyter-notebook-internals",
    "href": "01-intro/jupyter-notebook.html#jupyter-notebook-internals",
    "title": "Jupyter Notebooks",
    "section": "Jupyter Notebook Internals",
    "text": "Jupyter Notebook Internals\n\n\n\n\n\n\nCaution\n\n\n\nIf you have not yet tried interacting with your first Jupyter Notebook yet, this section will not make much sense. We recommend skipping ahead to the next set of notes, then coming back and referring to this as you build more notebooks.\n\n\nJupyter Notebooks are made up of cells. There are two main types of cells:\nCode cells. This is where you write and execute code. When run, Python code cells are evaluated as a Python code snippet, one line at a time. The cell output displayed is the value of the last evaluated expression:\n\n\n\n\n\n\n\nFigure 2: Both expressions are evaluated, but the result of the last expression’s evaluation is considered the output of the code cell.\n\n\n\n\nWe will discuss this output/display phenomenon more in future notes.\nTo run a code cell, you can either hit the “Run” button in the Toolbar, or you can use a keyboard shortcut: &lt;SHIFT&gt;+&lt;ENTER&gt;, which runs the cell and advances to the next cell. We recommend keyboard shortcuts; see below.\nMarkdown cells. This is where you write text and images that aren’t Python code. Markdown is a language used for formatting text. A Markdown cell will always display its formatting when it is not in edit mode.\n\n\n\n\n\n\n\nFigure 3: Left screenshot shows un-evaluated code cell and raw Markdown cell; right screenshot shows evaluated code cell and formatted text. To render formatted text for a selected markdown cell, exit editing mode for that cell. This screenshot starts with the code cell selected, then runs both that code cell and “runs” the markdown cell below.\n\n\n\n\nHere is a guide to Markdown formatting. You’ll explore Markdown more in lab.\n\nKeyboard Shortcuts\nWhile you can manage most of your notebook development by leveraging the Toolbar, many programmers (including me!) prefer using keyboard shortcuts. This minimizes use of the mouse/trackpad and keeps the hands on the keyboard. Together with stretching and taking breaks, keyboard shortcuts will reduce wrist cramps and improve your programming concentration.\nEdit mode vs. command mode: Hit the &lt;ESCAPE&gt; key on your keyboard to switch from edit mode to command mode. Keyboard shortcuts are specific to the mode you’re using:\n\nEdit mode: when you’re actively typing in the cell. Undo is &lt;CTRL/CMD&gt; + Z.\nCommand mode: when you’re not actively typing in the cell. Undo is z.\n\n\n\n\n\n\n\n\n\nAction\nMode\nKeyboard shortcut\n\n\n\n\nRun cell + jump to next cell*\nEither (puts you in edit mode)\n&lt;SHIFT&gt; + &lt;ENTER&gt;\n\n\nRun cell + stay on this cell\nEither (puts you in edit mode)\n&lt;CTRL/CMD&gt; + &lt;ENTER&gt;\n\n\nSave notebook\nEither\n&lt;CTRL/CMD&gt; + &lt;S&gt;\n\n\nSwitch to command mode*\nEither (puts you in command mode)\n&lt;ESCAPE&gt;\n\n\nSwitch to edit mode*\nCommand\n&lt;ENTER&gt;\n\n\nComment out the current line\nEdit\n&lt;CTRL/CMD&gt; + /\n\n\nCreate new cell above/below\nCommand\nA/B\n\n\nDelete cell\nCommand\nDD\n\n\nConvert cell to Markdown\nCommand\nM\n\n\nConvert cell to code\nCommand\nY\n\n\nShow all shortcuts\nCommand\nH\n\n\n\nThe above table should be used as a reference throughout the semester; don’t try to memorize these right now. And remember, you don’t have to use these shortcuts; you can always use the toolbar. Regardless, we’ve annotated the most useful keyboard shortcuts with an asterisk (*).\nThere are plenty more keyboard shortcuts available. Let us know if you find a good guide.",
    "crumbs": [
      "L01: Introduction",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "01-intro/index.html",
    "href": "01-intro/index.html",
    "title": "Course Introduction",
    "section": "",
    "text": "Check out our slides for today! No additional notes!",
    "crumbs": [
      "L01: Introduction"
    ]
  },
  {
    "objectID": "01-intro/index.html#what-will-i-learn-in-this-course",
    "href": "01-intro/index.html#what-will-i-learn-in-this-course",
    "title": "Course Introduction",
    "section": "",
    "text": "Check out our slides for today! No additional notes!",
    "crumbs": [
      "L01: Introduction"
    ]
  },
  {
    "objectID": "01-intro/index.html#external-reading",
    "href": "01-intro/index.html#external-reading",
    "title": "Course Introduction",
    "section": "External Reading",
    "text": "External Reading\n\nComputational and Inferential Thinking, Ch 1.1",
    "crumbs": [
      "L01: Introduction"
    ]
  },
  {
    "objectID": "06-variables-ii/sample-population.html",
    "href": "06-variables-ii/sample-population.html",
    "title": "Sample vs. Population",
    "section": "",
    "text": "Given a research question, the population is the group you want to learn something about However, directly studying the population as a whole is often not possible! Data might not exist at that scale, or it might be too costly to collect, if it’s even possible to gather that information.\nMany times, we instead study a sample of the population. If the sample is a good representation of the population, we can make useful analyses at a much lower cost.\n\n\nThe set of individuals we actually draw our sample from is the sampling frame. Depending on how we select our sample, we may miss individuals from the population we’re interested in, and we might also include individuals that are not in the population.",
    "crumbs": [
      "L06: Causality vs. EDA",
      "Sample vs. Population"
    ]
  },
  {
    "objectID": "06-variables-ii/sample-population.html#population-vs.-sample",
    "href": "06-variables-ii/sample-population.html#population-vs.-sample",
    "title": "Sample vs. Population",
    "section": "",
    "text": "Given a research question, the population is the group you want to learn something about However, directly studying the population as a whole is often not possible! Data might not exist at that scale, or it might be too costly to collect, if it’s even possible to gather that information.\nMany times, we instead study a sample of the population. If the sample is a good representation of the population, we can make useful analyses at a much lower cost.\n\n\nThe set of individuals we actually draw our sample from is the sampling frame. Depending on how we select our sample, we may miss individuals from the population we’re interested in, and we might also include individuals that are not in the population.",
    "crumbs": [
      "L06: Causality vs. EDA",
      "Sample vs. Population"
    ]
  },
  {
    "objectID": "06-variables-ii/sample-population.html#examples",
    "href": "06-variables-ii/sample-population.html#examples",
    "title": "Sample vs. Population",
    "section": "Examples",
    "text": "Examples\n\n\n\n\n\n\n\nFigure 1: A sampling frame may include individuals not in our population.\n\n\n\n\n\nExamples of target populations and the collected samples.\n\n\n\n\n\n\nTarget Population\nCollected sample\n\n\n\n\nStudent body of the school\nA specific classroom of students at the school\n\n\nA bag of 100 marbles\n10 marbles from the bag\n\n\nComputing Education Research (CER) papers\nPapers published at the American Society of Engineering Education (ASEE) conference\n\n\n\nIn the last example of the table, it is possible some of the research papers published at the ASEE conference are not specific to CER and may perhaps be focused on education in other engineering fields, like mechanical engineering or civil engineering. The sampling frame may be inferred to be the ASEE conference, and then the sample collected would need to be adjusted to include just the CER papers we want.\n\nA longer example\nLet’s say you’re planning a social event for all Data Science-declared sophomores (second-years). Since you only have the budget to cater pizza, you want to figure out what pizza toppings Data Science sophomores enjoy, and buy pizza toppings according to how popular they are.\nIn order to figure out the most popular flavors, you survey every student walking into or out of Warren Hall (where Data Science course office hours are located) from 12PM to 1PM by asking what their favorite topping is. Assume that everyone responds.\n\nPopulation: Data Science sophomores\nSampling frame: Students walking into/out of Warren Hall between 12PM and 1PM\n\nIf we draw a sample from this sampling frame we may not get a representative sample because we will get respondents from not just the sophomore pool, but also freshman, juniors, seniors, non-Data Science majors, and generally many more students than our target population. These students may have different preferences than Data Science sophomores.",
    "crumbs": [
      "L06: Causality vs. EDA",
      "Sample vs. Population"
    ]
  },
  {
    "objectID": "06-variables-ii/sample-population.html#how-do-we-construct-representative-samples",
    "href": "06-variables-ii/sample-population.html#how-do-we-construct-representative-samples",
    "title": "Sample vs. Population",
    "section": "How do we construct representative samples?",
    "text": "How do we construct representative samples?\nWe will (hopefully) get into this topic later this semester.",
    "crumbs": [
      "L06: Causality vs. EDA",
      "Sample vs. Population"
    ]
  },
  {
    "objectID": "06-variables-ii/index.html",
    "href": "06-variables-ii/index.html",
    "title": "Causality vs. EDA",
    "section": "",
    "text": "Read Inferential Thinking\n\n\n\nRead all of Chapter 2, which describes in detail experimental setup and design. It covers a core story to data scientists: John Snow and the Broad Street Pump.\nBefore continuing, make sure that you are familiar with the following terminology:\n\nobservational study\ncausality\nassociation\ncomparison\ntreatment group\ncontrol group\nrandomized controlled trial (RCT)",
    "crumbs": [
      "L06: Causality vs. EDA"
    ]
  },
  {
    "objectID": "06-variables-ii/index.html#john-snow-and-the-broad-street-pump",
    "href": "06-variables-ii/index.html#john-snow-and-the-broad-street-pump",
    "title": "Causality vs. EDA",
    "section": "",
    "text": "Read Inferential Thinking\n\n\n\nRead all of Chapter 2, which describes in detail experimental setup and design. It covers a core story to data scientists: John Snow and the Broad Street Pump.\nBefore continuing, make sure that you are familiar with the following terminology:\n\nobservational study\ncausality\nassociation\ncomparison\ntreatment group\ncontrol group\nrandomized controlled trial (RCT)",
    "crumbs": [
      "L06: Causality vs. EDA"
    ]
  },
  {
    "objectID": "06-variables-ii/index.html#randomized-controlled-trials-vs.-observational-studies",
    "href": "06-variables-ii/index.html#randomized-controlled-trials-vs.-observational-studies",
    "title": "Causality vs. EDA",
    "section": "Randomized Controlled Trials vs. Observational Studies",
    "text": "Randomized Controlled Trials vs. Observational Studies\nIn the Broad Street Pump experiment, John Snow established a causal relationship (between what? Read Inferential Thinking Chapter 2 to find out) because he noted that there was no systematic difference between the two different groups observed other than along a single variable dimension.\nIn modern days, randomized controlled trials are excellent ways to compare two groups of otherwise similar individuals. However, in the majority of this class we will not be able to conduct a randomized controlled trial. This is because the datasets we analyze are almost all observational studies and not experiments. Moreover, these datasets are largely pre-existing materials collected by other researchers, and we may not know the entire picture of how they collected the data. As a result, in this class we seek to understand associations between variables, and we will almost never seek to establish causal relationships between variables.\nTo further understand causality, we encourage you to take inferential thinking courses like Data 8, Stat 20, and a wide range of Statistics courses.",
    "crumbs": [
      "L06: Causality vs. EDA"
    ]
  },
  {
    "objectID": "06-variables-ii/index.html#confounding",
    "href": "06-variables-ii/index.html#confounding",
    "title": "Causality vs. EDA",
    "section": "Confounding",
    "text": "Confounding\nFrom Inferential Thinking, Ch 3.2 Establishing Causality:\n\nIn an observational study, if the treatment and control groups differ in ways other than the treatment, it is difficult to make conclusions about causality.\nAn underlying difference between the two groups (other than the treatment) is called a confounding factor, because it might confound you (that is, mess you up) when you try to reach a conclusion.\n\nConfounding occurs when two variables can be consistently associated with each other even when one does not cause the other.\nTo determine whether a confounding variable can account for the association between two variables, we can try to disaggregate by different values of the confounding variable.\nThis disaggregation process can be repeated exhaustively for a potentially infinite number of confounding variables. Researchers generally don’t do this. Instead, we usually rely on assumptions drawn from social science theory or findings from prior studies. This process can narrow our search of potential confounding variables that may influence the association between two variables.",
    "crumbs": [
      "L06: Causality vs. EDA"
    ]
  },
  {
    "objectID": "06-variables-ii/index.html#exploratory-data-analysis",
    "href": "06-variables-ii/index.html#exploratory-data-analysis",
    "title": "Causality vs. EDA",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nIf we’re not going to be studying causal relationships in this class, what will we be looking at? In this course we will look deeply at a core component of Data Science: Exploratory Data Analysis, or EDA.\nExploratory Data Analysis (EDA) is like detective work. As coined by the famous American statistician and mathematician John Tukey (we will discuss Tukey numbers soon):\n\nExploratory data analysis is an attitude, a state of flexibility, a willingness to look for those things that we believe are not there, as well as those that we believe to be there.\n\nMore formally, Exploratory Data Analysis (EDA) is the process of understanding a new dataset. It is an open-ended, informal analysis that involves familiarizing ourselves with the variables present in the data, discovering potential hypotheses, and identifying possible issues with the data.",
    "crumbs": [
      "L06: Causality vs. EDA"
    ]
  },
  {
    "objectID": "06-variables-ii/index.html#data-wrangling",
    "href": "06-variables-ii/index.html#data-wrangling",
    "title": "Causality vs. EDA",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nA process very closely related to EDA is data wrangling, often called data cleaning. Data wrangling is the process of transforming raw data to facilitate subsequent analysis and can address issues like unclear structure or formatting, missing or corrupted values, unit conversions, and so on.\nEDA and data cleaning are often thought of as an “infinite loop,” with each process driving the other.\nFortunately, in our classes we will try our best to work with “clean” datasets. These datasets will often have already been preprocessed for cleaner analysis, allowing us to explore and ask questions much more easily than if we were stuck with messier data.",
    "crumbs": [
      "L06: Causality vs. EDA"
    ]
  },
  {
    "objectID": "06-variables-ii/index.html#external-reading",
    "href": "06-variables-ii/index.html#external-reading",
    "title": "Causality vs. EDA",
    "section": "External Reading",
    "text": "External Reading\n\n(mentioned in notes) Computational and Inferential Thinking, Ch 5.1\n“Chapter 15: From Concepts to Models.” Elizabeth Heger Boyle, Deborah Carr, Benjamin Cornwell, Shelley Correll, Robert Crosnoe, Jeremy Freese, and Waters, Mary C. 2017. The Art and Science of Social Research. New York: W. W. Norton & Company.",
    "crumbs": [
      "L06: Causality vs. EDA"
    ]
  },
  {
    "objectID": "04-tables/index.html",
    "href": "04-tables/index.html",
    "title": "Table Fundamentals",
    "section": "",
    "text": "Read Inferential Thinking\n\n\n\nRead Ch 6 intro, which describes the Table object type from the datascience library.\nThis is a very dense chapter, as it has lots of Python syntax. We will review it in detail. Before moving on, we encourage you to focus on the following questions:\n\nWhat is the syntax of the with_columns method? How does this method make a table?\nWhat are some table methods?\nWhat Python expression gets you the number of rows? a specific column? (bonus) How does the syntax differ between these two cases?\nIf you are new to programming, it is easy to get lost in the syntax of Python! For today, we will therefore focus on a few high-level goals:\nIn other words, you will not be expected to memorize all aspects of Tables (it’s as if we have a theme in this class…)! But you will have natural impulses for understanding and working with tabular data, many of which directly map to specific Table methods and attributes. To write Python code, then, you will need to learn to work with documentation to do this translation, and—when the precise methods don’t exist—construct new algorithms to achieve what you want. The more familiar you are with the documentation—and consequently, what is possible with Tables—the more you can focus on algorithmic thinking.",
    "crumbs": [
      "L04: Table Fundamentals"
    ]
  },
  {
    "objectID": "04-tables/index.html#definitions",
    "href": "04-tables/index.html#definitions",
    "title": "Table Fundamentals",
    "section": "Definitions",
    "text": "Definitions\nTo begin, recall the terminology we have been using to describe tables so far:\n\nTable: Retangular data structure.\n\nColumns (vertical) correspond to variables (AKA features, or attributes) that measure and operationalize social concepts. We discuss variables and social concepts more in another note.\nRows (horizontal) correspond to records (AKA entries), which are specific values of variables for a given individual, group, etc.—whatever your unit of analysis is.\n\n\nWe work with the datascience package’s Table.\n\nfrom datascience import *\nimport numpy as np",
    "crumbs": [
      "L04: Table Fundamentals"
    ]
  },
  {
    "objectID": "04-tables/index.html#todays-dataset-schools",
    "href": "04-tables/index.html#todays-dataset-schools",
    "title": "Table Fundamentals",
    "section": "Today’s Dataset: Schools",
    "text": "Today’s Dataset: Schools\nThis lecture we will return to the dataset of public universities in California (Wikipedia).\nTo work with this table, we first load it in from a file called data/cal_unis.csv.\n\nschools = Table.read_table('data/cal_unis.csv')\n\nLet’s break down what we meant by “load it in”: * The right-hand-side of the assignment calls a function that returns a datascience Table object from the provided data file. * The left-hand-side of the assignment assigns this object to the Python name schools. * After this assignment statement, then we can use schools as the particular Table object that has the tabular data we want.\nRunning the below cell evaluates schools and displays that tabular data to us:\n\nschools\n\n\n\n\nName\nInstitution\nCity\nCounty\nEnrollment\nFounded\n\n\n\n\nCalifornia State Polytechnic University, Humboldt\nCSU\nArcata\nHumboldt\n6025\n1913\n\n\nCalifornia State University, Bakersfield\nCSU\nBakersfield\nKern\n9613\n1965\n\n\nUniversity of California, Berkeley\nUC\nBerkeley\nAlameda\n45307\n1869\n\n\nCalifornia State University Channel Islands\nCSU\nCamarillo\nVentura\n6128\n2002\n\n\nCalifornia State University, Dominguez Hills\nCSU\nCarson\nLos Angeles\n16426\n1960\n\n\nCalifornia State University, Chico\nCSU\nChico\nButte\n14183\n1887\n\n\nUniversity of California, Davis\nUC\nDavis\nYolo\n39679\n1905\n\n\nCalifornia State University, Fresno\nCSU\nFresno\nFresno\n23999\n1911\n\n\nCalifornia State University, Fullerton\nCSU\nFullerton\nOrange\n40386\n1957\n\n\nCalifornia State University, East Bay\nCSU\nHayward\nAlameda\n13673\n1957\n\n\n\n\n... (22 rows omitted)\n\n\nLet’s get to exploring and understanding our tabular data!",
    "crumbs": [
      "L04: Table Fundamentals"
    ]
  },
  {
    "objectID": "04-tables/index.html#find-table-dimensions",
    "href": "04-tables/index.html#find-table-dimensions",
    "title": "Table Fundamentals",
    "section": "Find Table Dimensions",
    "text": "Find Table Dimensions\nFirst off: How many rows and columns are in our schools table?\n\nprint(schools.num_rows)\nprint(schools.num_columns)\n\n32\n6\n\n\nWhat variables are being recorded?\n\nprint(schools.labels)\n\n('Name', 'Institution', 'City', 'County', 'Enrollment', 'Founded')\n\n\nWe didn’t conjure these expressions out of nowhere. Instead, we:\n\nLooked at the Data 6 Python Reference page\nScrolled down to the “Tables and Table Methods” section\nSkimmed the reference until we found documentation that seemed close to what we want\nTranslated the name tbl into schools.\nTried it out\n\nBonus: You’ll notice the “dot” syntax for accessing these values, also known as table attributes. See the Bonus last section for a more detailed explanation of this programming terminology.",
    "crumbs": [
      "L04: Table Fundamentals"
    ]
  },
  {
    "objectID": "04-tables/index.html#column-first-paradigm",
    "href": "04-tables/index.html#column-first-paradigm",
    "title": "Table Fundamentals",
    "section": "Column-First Paradigm",
    "text": "Column-First Paradigm\nBecause columns represent variables, we will take a column-first approach to tables.\nTo extract, delete, or make columns from a table named tbl:\n\ntbl.select(...) returns a new table with a subset of columns.\ntbl.drop(...) and returns a new table without a subset of columns\ntbl.with_columns(...) returns a new table with additional new column(s).\n\n\nschools.select('Name', 'Enrollment')\n\n\n\n\nName\nEnrollment\n\n\n\n\nCalifornia State Polytechnic University, Humboldt\n6025\n\n\nCalifornia State University, Bakersfield\n9613\n\n\nUniversity of California, Berkeley\n45307\n\n\nCalifornia State University Channel Islands\n6128\n\n\nCalifornia State University, Dominguez Hills\n16426\n\n\nCalifornia State University, Chico\n14183\n\n\nUniversity of California, Davis\n39679\n\n\nCalifornia State University, Fresno\n23999\n\n\nCalifornia State University, Fullerton\n40386\n\n\nCalifornia State University, East Bay\n13673\n\n\n\n\n... (22 rows omitted)\n\n\n\nschools.drop('Founded', 'County')\n\n\n\n\nName\nInstitution\nCity\nEnrollment\n\n\n\n\nCalifornia State Polytechnic University, Humboldt\nCSU\nArcata\n6025\n\n\nCalifornia State University, Bakersfield\nCSU\nBakersfield\n9613\n\n\nUniversity of California, Berkeley\nUC\nBerkeley\n45307\n\n\nCalifornia State University Channel Islands\nCSU\nCamarillo\n6128\n\n\nCalifornia State University, Dominguez Hills\nCSU\nCarson\n16426\n\n\nCalifornia State University, Chico\nCSU\nChico\n14183\n\n\nUniversity of California, Davis\nUC\nDavis\n39679\n\n\nCalifornia State University, Fresno\nCSU\nFresno\n23999\n\n\nCalifornia State University, Fullerton\nCSU\nFullerton\n40386\n\n\nCalifornia State University, East Bay\nCSU\nHayward\n13673\n\n\n\n\n... (22 rows omitted)\n\n\nNote: The above two cells were run in sequence. Each table method returns a new table, so our original table schools is unchanged.\n\nModifying Tables\n\n\n\n\n\n\nTable methods return copies\n\n\n\nAll table methods return copies of information from the original table! This paradigm is quite useful for data analysis. From Inferential Thinking:\n\n[Table methods] create new smaller tables that share the same data. The fact that the original table is preserved is useful! You can generate multiple different tables that only consider certain columns without worrying that one analysis will affect the other.\n\n\n\nIf we would like to modify the original table, then we must re-assign schools to the new return value:\n\nschools = schools.with_columns(\n    \"Years since founding\",\n    2025 - schools.column(\"Founded\")\n)\nschools\n\n\n\n\nName\nInstitution\nCity\nCounty\nEnrollment\nFounded\nYears since founding\n\n\n\n\nCalifornia State Polytechnic University, Humboldt\nCSU\nArcata\nHumboldt\n6025\n1913\n112\n\n\nCalifornia State University, Bakersfield\nCSU\nBakersfield\nKern\n9613\n1965\n60\n\n\nUniversity of California, Berkeley\nUC\nBerkeley\nAlameda\n45307\n1869\n156\n\n\nCalifornia State University Channel Islands\nCSU\nCamarillo\nVentura\n6128\n2002\n23\n\n\nCalifornia State University, Dominguez Hills\nCSU\nCarson\nLos Angeles\n16426\n1960\n65\n\n\nCalifornia State University, Chico\nCSU\nChico\nButte\n14183\n1887\n138\n\n\nUniversity of California, Davis\nUC\nDavis\nYolo\n39679\n1905\n120\n\n\nCalifornia State University, Fresno\nCSU\nFresno\nFresno\n23999\n1911\n114\n\n\nCalifornia State University, Fullerton\nCSU\nFullerton\nOrange\n40386\n1957\n68\n\n\nCalifornia State University, East Bay\nCSU\nHayward\nAlameda\n13673\n1957\n68\n\n\n\n\n... (22 rows omitted)",
    "crumbs": [
      "L04: Table Fundamentals"
    ]
  },
  {
    "objectID": "04-tables/index.html#filtering-rows",
    "href": "04-tables/index.html#filtering-rows",
    "title": "Table Fundamentals",
    "section": "Filtering Rows",
    "text": "Filtering Rows\nOften we would like to perform row filtering, where we only extract row entries that match a specific feature.\nBy exact match: tbl.where(column, value). Create a new table of only the rows where column column matches the value value.\n\nschools.where(\"Institution\", \"UC\")\n\n\n\n\nName\nInstitution\nCity\nCounty\nEnrollment\nFounded\nYears since founding\n\n\n\n\nUniversity of California, Berkeley\nUC\nBerkeley\nAlameda\n45307\n1869\n156\n\n\nUniversity of California, Davis\nUC\nDavis\nYolo\n39679\n1905\n120\n\n\nUniversity of California, Irvine\nUC\nIrvine\nOrange\n35937\n1965\n60\n\n\nUniversity of California, Los Angeles\nUC\nLos Angeles\nLos Angeles\n46430\n1882\n143\n\n\nUniversity of California, Merced\nUC\nMerced\nMerced\n9110\n2005\n20\n\n\nUniversity of California, Riverside\nUC\nRiverside\nRiverside\n26809\n1954\n71\n\n\nUniversity of California, San Diego\nUC\nSan Diego\nSan Diego\n42006\n1960\n65\n\n\nUniversity of California, Santa Barbara\nUC\nSanta Barbara\nSanta Barbara\n26420\n1891\n134\n\n\nUniversity of California, Santa Cruz\nUC\nSanta Cruz\nSanta Cruz\n19478\n1965\n60\n\n\n\n\n\nBy index: tbl.take(row_indices). Create a new table of only the rows with an index specified in row_indices.\n\nschools.take(1, 3, 5)\n\n\n\n\nName\nInstitution\nCity\nCounty\nEnrollment\nFounded\nYears since founding\n\n\n\n\nCalifornia State University, Bakersfield\nCSU\nBakersfield\nKern\n9613\n1965\n60\n\n\nCalifornia State University Channel Islands\nCSU\nCamarillo\nVentura\n6128\n2002\n23\n\n\nCalifornia State University, Chico\nCSU\nChico\nButte\n14183\n1887\n138\n\n\n\n\n\nAre rows zero-indexed or one-indexed? Why?",
    "crumbs": [
      "L04: Table Fundamentals"
    ]
  },
  {
    "objectID": "04-tables/index.html#creating-new-tables",
    "href": "04-tables/index.html#creating-new-tables",
    "title": "Table Fundamentals",
    "section": "Creating New Tables",
    "text": "Creating New Tables\nSo far we’ve used a table with pre-existing data. We can also make new Tables with a special function: Table().\n\nTable()\n\n\n\n\n\n\n\nCheck it out!!!\n\nstates = Table().with_columns('State', np.array(['California', 'New York', 'Florida', 'Texas', 'Pennsylvania']),'Code', np.array(['CA', 'NY', 'FL', 'TX', 'PA']), 'Population (millions)', np.array([39.3, 19.3, 21.7, 29.3, 12.8]))\nstates\n\n\n\n\nState\nCode\nPopulation (millions)\n\n\n\n\nCalifornia\nCA\n39.3\n\n\nNew York\nNY\n19.3\n\n\nFlorida\nFL\n21.7\n\n\nTexas\nTX\n29.3\n\n\nPennsylvania\nPA\n12.8\n\n\n\n\n\n…unfortunately, due to poor coding style, the above code is quite unreadable. Let’s use whitespace (new lines and indents) to delineate what we are doing:\n\nstates = Table().with_columns(\n  'State', make_array('California', 'New York', 'Florida', 'Texas', 'Pennsylvania'),\n  'Code', make_array('CA', 'NY', 'FL', 'TX', 'PA'),\n  'Population (millions)', make_array(39.3, 19.3, 21.7, 29.3, 12.8)\n)\nstates\n\n\n\n\nState\nCode\nPopulation (millions)\n\n\n\n\nCalifornia\nCA\n39.3\n\n\nNew York\nNY\n19.3\n\n\nFlorida\nFL\n21.7\n\n\nTexas\nTX\n29.3\n\n\nPennsylvania\nPA\n12.8\n\n\n\n\n\n\nMethod Chaining\nMethod chaining in Python is when the object returned from one method becomes the object to use in the next method. In the above case:\n\nCalling the Table returns an empty table.\nThe method with_columns is called on the empty table object, returning a table with the columns State, Code, and Population.\nThis return value is then assigned to states.\n\n\n\n\n\n\n\nWhitespace formatting\n\n\n\nNote the closed parenthesis on the final line. This helps “match” parentheses together.",
    "crumbs": [
      "L04: Table Fundamentals"
    ]
  },
  {
    "objectID": "04-tables/index.html#bonus-methods-vs.-attributes",
    "href": "04-tables/index.html#bonus-methods-vs.-attributes",
    "title": "Table Fundamentals",
    "section": "Bonus: Methods vs. Attributes",
    "text": "Bonus: Methods vs. Attributes\nReturn to the third focus question from the Inferential Thinking reading.\n\nWhat Python expression gets you the number of rows? a specific column? (bonus) How does the syntax differ between these two cases?\n\nThe textbook wording “overloads” two different Python syntax constructs. From the Python glossary:\n\n\n\n\n\n\nAttribute\n\n\n\nAttribute: A value associated with an object which is usually referenced by name using dotted expressions. For example, if an object o has an attribute a it would be referenced as o.a.\nIn other words, attributes are named values tied to specific objects. To get these values, we must refer to them by their name using “dot” syntax.\n\n\n\nschools.num_rows\n\n32\n\n\n\n\n\n\n\n\nMethod\n\n\n\nMethod A function which is defined inside a class body.\nThe Python glossary definition above is a more opaque definition than we would like. But you can think of the methods we study in this class as functions that take into account the attributes and values of specific objects. In order to call them on a specific operation, we also use “dot” syntax. Because they are functions, we must use call expression syntax with parenthesis, and provide arguments as needed.\n\n\n\nschools.column(\"Enrollment\")\n\narray([ 6025,  9613, 45307,  6128, 16426, 14183, 39679, 23999, 40386,\n       13673, 35937, 38973, 46430, 26460,  9110, 37579, 27503, 26809,\n        6637, 31818, 19803, 42006, 37402, 25282, 35751, 22000, 15109,\n       26420, 19478,  7045, 10154,  1017])\n\n\nWhich to syntax use? To determine which “dot” syntax to use, read the documentation. The entire Table class has been defined for us already, and the designers of the datascience package have predetermined which table features make sense to have as attributes, and which features make sense to return as values from a method call. But as a rule of thumb, if you need to specify any additional detail to get what you want, you will likely need to call a function: use table methods and pass in arguments. See the column method call above.\nAnother key indicator is what happens when you use incorrect syntax:\n\nschools.num_rows()\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[16], line 1\n----&gt; 1 schools.num_rows()\n\nTypeError: 'int' object is not callable\n\n\n\nThe above error message means that Python assumed you wanted to call a function num_rows (specifically, a method of the Table schools); however, when run, num_rows was actually an int data type attribute, and ints are not callable. Similarly, below:\n\nschools.column\n\n&lt;bound method Table.column of Name                                              | Institution | City        | County      | Enrollment | Founded | Years since founding\nCalifornia State Polytechnic University, Humboldt | CSU         | Arcata      | Humboldt    | 6025       | 1913    | 112\nCalifornia State University, Bakersfield          | CSU         | Bakersfield | Kern        | 9613       | 1965    | 60\nUniversity of California, Berkeley                | UC          | Berkeley    | Alameda     | 45307      | 1869    | 156\nCalifornia State University Channel Islands       | CSU         | Camarillo   | Ventura     | 6128       | 2002    | 23\nCalifornia State University, Dominguez Hills      | CSU         | Carson      | Los Angeles | 16426      | 1960    | 65\nCalifornia State University, Chico                | CSU         | Chico       | Butte       | 14183      | 1887    | 138\nUniversity of California, Davis                   | UC          | Davis       | Yolo        | 39679      | 1905    | 120\nCalifornia State University, Fresno               | CSU         | Fresno      | Fresno      | 23999      | 1911    | 114\nCalifornia State University, Fullerton            | CSU         | Fullerton   | Orange      | 40386      | 1957    | 68\nCalifornia State University, East Bay             | CSU         | Hayward     | Alameda     | 13673      | 1957    | 68\n... (22 rows omitted)&gt;\n\n\nThe above statement did not error, but it did not output a column, either. Rather, it output a method of a table, but it didn’t call this method.",
    "crumbs": [
      "L04: Table Fundamentals"
    ]
  },
  {
    "objectID": "04-tables/none-print.html",
    "href": "04-tables/none-print.html",
    "title": "None and Print",
    "section": "",
    "text": "We have covered a few data types so far. There are infinitely* many integers, floating point numbers, and strings. (*Actually finite, because of how computers store information. Take CS61C to learn more.)\nHowever, for the NoneType data type, there is only one value: None.\n\nmy_var = None\ntype(my_var)\n\nNoneType\n\n\nNone is strange: Cells will not output expressions that evaluate to None.\nNone is also referred to as the null value.",
    "crumbs": [
      "L04: Table Fundamentals",
      "None and Print"
    ]
  },
  {
    "objectID": "04-tables/none-print.html#none-type",
    "href": "04-tables/none-print.html#none-type",
    "title": "None and Print",
    "section": "",
    "text": "We have covered a few data types so far. There are infinitely* many integers, floating point numbers, and strings. (*Actually finite, because of how computers store information. Take CS61C to learn more.)\nHowever, for the NoneType data type, there is only one value: None.\n\nmy_var = None\ntype(my_var)\n\nNoneType\n\n\nNone is strange: Cells will not output expressions that evaluate to None.\nNone is also referred to as the null value.",
    "crumbs": [
      "L04: Table Fundamentals",
      "None and Print"
    ]
  },
  {
    "objectID": "04-tables/none-print.html#the-print-function",
    "href": "04-tables/none-print.html#the-print-function",
    "title": "None and Print",
    "section": "The print function",
    "text": "The print function\nThe print function displays values to the screen (in this case, our Jupyter notebook). Each call to print displays information on a new line.\n\nprint(2)\nprint(\"Hello, world!\")\n\n2\nHello, world!\n\n\nprint lets us to see information about executed statements that are not just the last line of the cell. Could you see why this would be a useful feature for debugging?\nprint is ultra-convenient because it can take as many arguments as you want.\n\nx = 3\ny = 4\nprint(x, \"+\", y, \"is equal to\", x + y)\n\n3 + 4 is equal to 7\n\n\nTrace through the above program. Do you see how expressions are evaluated before printing?",
    "crumbs": [
      "L04: Table Fundamentals",
      "None and Print"
    ]
  },
  {
    "objectID": "04-tables/none-print.html#notebooks-cell-output-vs.-print",
    "href": "04-tables/none-print.html#notebooks-cell-output-vs.-print",
    "title": "None and Print",
    "section": "Notebooks: Cell Output vs. print",
    "text": "Notebooks: Cell Output vs. print\nCell output and print output seem very closely related: * Both evaluate expressions * Both display something in the notebook\nHowever, they have subtle differences. We won’t expect you to know all of them. Instead, the purpose of this section is to get you used to tracing programs, where you closely consider Python’s order of execution of each statement, expression, function call, and so on.\nBecause the print output is intended for human view, it displays strings without quotes. Contrast this with the cell output, which includes the quotes.\n\nprint(2)\n\"Hello, world!\"\n\n2\n\n\n'Hello, world!'\n\n\n\nExercise 1\nThis is now super pedantic, but consider the following cells. Can you explain what is happening?\n\nmy_var\n\n\nprint(my_var)\n\nNone\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nWhen print is evaluated as the last line in a cell, it displays the value of the evaluated argument (here, None). print returns None, so the cell does not additionally output anything,\n\n\n\n\n\nExercise 2\nTry this challenge on for size.\n\nprint(my_var)\nprint(3)\n45\n\nNone\n3\n\n\n45\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nEach line explained:\n\nEvaluate the first print call expression. This prints the value of my_var to the screen, which evaluates to None. print returns None, but this second None is not output to the sceen because it is not the last line.\nEvaluate the second print call expression. This prints the value of 3 to the screen. print returns None, but this second None is not output to the sceen because it is not the last line.\nEvaluate the third expression, 45; because it is the last line, output to screen.\n\n\n\n\nYou’re thinking like a computer scientist now!",
    "crumbs": [
      "L04: Table Fundamentals",
      "None and Print"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "STAT 133 is an introductory-to-intermediate level course to computational data analysis with an emphasis on four major cornerstones:\n\n🔢 Understand common data formats, and principles of data manipulation (e.g. wrangling, reshaping, tidying)\n📊 Production of data visualizations and their role in data analysis projects\n💻 Learn basic principles for writing code, and programming concepts (with emphasis on data analysis)\n⚒️ Use computational tools to carry out the data analysis cycle, organize your workflow, and become familiar with reporting tools (via dynamic documents and web-apps)"
  },
  {
    "objectID": "syllabus.html#description",
    "href": "syllabus.html#description",
    "title": "Syllabus",
    "section": "",
    "text": "STAT 133 is an introductory-to-intermediate level course to computational data analysis with an emphasis on four major cornerstones:\n\n🔢 Understand common data formats, and principles of data manipulation (e.g. wrangling, reshaping, tidying)\n📊 Production of data visualizations and their role in data analysis projects\n💻 Learn basic principles for writing code, and programming concepts (with emphasis on data analysis)\n⚒️ Use computational tools to carry out the data analysis cycle, organize your workflow, and become familiar with reporting tools (via dynamic documents and web-apps)"
  },
  {
    "objectID": "syllabus.html#goals",
    "href": "syllabus.html#goals",
    "title": "Syllabus",
    "section": "🎯 Goals",
    "text": "🎯 Goals\nBy the end of the course, students will:\n\nConstruct and execute basic programs in R using elementary programming techniques and tidyverse packages.\nVisualize information and data using appropriate graphical techniques.\nImport data from files or the internet.\nMunge raw data into a tidy format.\nCreate visualizations using geospatial data.\nParse and analyze text documents.\nCreate reproducible documents.\nConstruct interactive web applications."
  },
  {
    "objectID": "syllabus.html#expectations",
    "href": "syllabus.html#expectations",
    "title": "Syllabus",
    "section": "🤞 Expectations",
    "text": "🤞 Expectations\nThis is a computational data analysis course rather than a math or statistics or general programming course. Scripting-based data analysis is a broad topic, so it’s not possible for lecture to provide you with a specific recipe for every situation you may encounter.\nBecause STAT 133 is one of the core courses for Statistics majors, the underlying intention is to provide foundations for “computing with data” so that stat majors, as well as other data-dependent majors (e.g. Data Science, Applied Math, CogSci, Economics, etc), have the basic computational skills for subsequent upper division courses (e.g. STAT 150, 151A, 152, 153, 154, 155, 157, 158, 159).\nWe don’t expect that you become a jedi data scientist, an R ninja, or a super coder. That takes YEARS of practice, training, learning, and collaboration. Instead, we want to give you a good foundation around tools for computational data analysis."
  },
  {
    "objectID": "syllabus.html#ℹ-prerequisites",
    "href": "syllabus.html#ℹ-prerequisites",
    "title": "Syllabus",
    "section": "ℹ️ Prerequisites",
    "text": "ℹ️ Prerequisites\nThis course does not have any prerequisites, although it would be nice if you have taken an introductory course in statistics (e.g. STAT 2, 20, 21, 131A).\nThe curriculum and format is designed specifically for students (ideally majoring in Statistics or minoring in Data Science) who have no-or-minimum programming experience. You also don’t need previous data analysis experience—although it helps if you do.\nStudents with some prior experience in either computational statistics or computing are welcome to enroll, though some parts of the course might feel extremely slow. We recommend that you take more advanced courses unless you need STAT 133 because of your major/minor’s requirements.\n\n\n\nCartoon by John McPherson"
  },
  {
    "objectID": "syllabus.html#course-culture",
    "href": "syllabus.html#course-culture",
    "title": "Syllabus",
    "section": "🏫 Course Culture",
    "text": "🏫 Course Culture\nStudents taking STAT 133 come from a wide range of backgrounds. We hope to foster an inclusive and supportive learning environment based on curiosity rather than competition. All members of the course community—the instructor, GSIs, students, tutors, and readers—are expected to treat each other with courtesy and respect.\nYou will be interacting with course staff and fellow students in several different environments: in class, in lab, over the discussion forum, and in office hours. Some of these will be in person, some of them will be online, but the same expectations hold: be kind, be respectful, be professional.\nIf you are concerned about classroom environment issues created by other students or course staff, please come talk to us about it."
  },
  {
    "objectID": "syllabus.html#textbooks",
    "href": "syllabus.html#textbooks",
    "title": "Syllabus",
    "section": "📚 Textbooks",
    "text": "📚 Textbooks\nWe’ll be using a handful of textbooks (most of them based on the notes I’ve authored for STAT 133 in the last 9 years):\n\nhttps://www.gastonsanchez.com/R-coding-basics/\nhttps://www.gastonsanchez.com/R-tidy-hurricanes/\nhttps://www.gastonsanchez.com/R-rolling-dice/\nhttps://www.gastonsanchez.com/R-for-strings/\nhttps://www.gastonsanchez.com/R-web-technologies/\nhttps://www.tidytextmining.com/ (by Julia Silge and David Robinson)"
  },
  {
    "objectID": "syllabus.html#computational-tools",
    "href": "syllabus.html#computational-tools",
    "title": "Syllabus",
    "section": "🔧 Computational Tools",
    "text": "🔧 Computational Tools\nWe will be mainly using the computing and programming environment R (via RStudio) to analyze data in this class. We may also ask you to use a command line interface to interact with your operating system. You do need your own computer to use R and do the assignments.\nWe don’t expect that students have already been exposed to R. For those who come from Data 8 or some previous coding experience in python, the first labs will help you transfer that knowledge over to R. Both languages are excellent platforms for analyzing data, are widely used in data science, and have their individual strengths. R has been developed within the statistics community specifically for data analysis, while python is a general-purpose programming language but has large data analysis capabilities.\nThe lectures tend to be focused on the data analysis concepts, while learning how to apply/use them in R tend to be the focus of the labs and assignments. Do not be surprised or worried about not following the details of R code during class—that is not the point (furthermore, there will sometimes be code that is really specific to the instructional purposes of the lecture, e.g. to make a specific plot, and is beyond the scope of what you would be expected to understand or know how to do it by yourself)."
  },
  {
    "objectID": "syllabus.html#waitlisted-students-and-late-joining",
    "href": "syllabus.html#waitlisted-students-and-late-joining",
    "title": "Syllabus",
    "section": "⏳ Waitlisted Students and Late Joining",
    "text": "⏳ Waitlisted Students and Late Joining\nIf you are on the waiting list or have a pending application or added the course late, you must still do all coursework and complete labs and homework by the deadlines. We will not be offering extensions/exceptions if you are admitted/enrolled into the course later. So it is your responsibility to stay up to date on the assignments.\nUnfortunately, doing all the work is not a guarantee of enrollment. You will only be enrolled if there is space in your lab. Enrollment will proceed by CalCentral."
  },
  {
    "objectID": "syllabus.html#office-hours",
    "href": "syllabus.html#office-hours",
    "title": "Syllabus",
    "section": "🚪 Office hours",
    "text": "🚪 Office hours\nMe (the instructor) and the GSIs will offer office hours each week across a range of times. You are welcome to visit the office hours of any instructor, not just the ones of your GSI. We may adjust the office hour schedule throughout the semester as we understand student needs and preferences. Please check the office hours tab on the staff page to see the times of the various OH sessions.\nI should also say that OH are an opportunity to chat one-on-one with me. If you can, please come to my office hours! Coming to OH does not necessarily send a signal that you are behind or need extra help. On the contrary, coming to office hours early and often tends to co-occur with success in the course. I am happy to chat about the course material, statistics in general, careers in statistics, and whatever other statistics or data science topics are on your mind!"
  },
  {
    "objectID": "syllabus.html#group-tutoring",
    "href": "syllabus.html#group-tutoring",
    "title": "Syllabus",
    "section": "🎒 Group Tutoring",
    "text": "🎒 Group Tutoring\nTutors will offer group tutoring sessions several times each week. This is an opportunity to finish up any assignments that you’ve started in class or review any topics that are confusing for you. You’re welcome to attend any session that works well for your schedule.\nGroup tutoring is a great place to go to meet other students and collaborate on assignments with tutors on hand to help you get unstuck."
  },
  {
    "objectID": "syllabus.html#labs-10-of-final-grade",
    "href": "syllabus.html#labs-10-of-final-grade",
    "title": "Syllabus",
    "section": "🔬 Labs (10% of final grade)",
    "text": "🔬 Labs (10% of final grade)\n\nWeekly lab discussions are an essential part of the course and we will introduce concepts not necessarily covered in class.\nThursdays are the official days for lab section.\nYou must attend the lab section you are officially enrolled in.\nDuring lab, you will work on short-form assignments designed to apply the concepts on real and simulated data sets.\nWe have designed the labs to be completed within the allotted time (~2 hrs), assuming that you attend section.\nLab assignments will be released every Thursday (available in bCourses),\nThe due date is always on a Friday (please check the assignments tab in bCourses to keep track of deadlines).\nSolutions to lab assignments will be available a few days after their due date.\nWe will be giving credit on lab assignments based on completion.\nSubmissions within 24 hours after the deadline will receive a 15% deduction. Submissions that are 24 hours or more after the deadline will receive no credit.\nThe first lab assignment (lab-1) does not count toward your grade.\nOf all lab assignments (lab-2 to lab-14), your lowest 2 scores will be dropped in the calculation of your overall grade."
  },
  {
    "objectID": "syllabus.html#problem-sets-35-of-final-grade",
    "href": "syllabus.html#problem-sets-35-of-final-grade",
    "title": "Syllabus",
    "section": "📁 Problem Sets (35% of final grade)",
    "text": "📁 Problem Sets (35% of final grade)\n\nThere will be 6 Problem-Set (PS) assignments (available in bCourses).\nPS are long-form assignments designed to apply the concepts you’ve learned in class and lab.\nStarting on week-2 they will be assigned every week, until week-7.\nThe due date is always on a Friday (please check the assignments tab in bCourses to keep track of deadlines).\nYou must write your own answers (using your own words and/or code). Copy and plagiarism will not be tolerated (see Academic Honesty policy).\nIf you don’t submit all required files, you will receive an automatic 10% deduction.\nIf you submit the incorrect files, you will receive no credit.\nSolutions will become available a few days (e.g. 3-4 days) after the due date.\nWe will drop the lowest Problem-Set assignment score in the calculation of your overall grade."
  },
  {
    "objectID": "syllabus.html#app-projects-27-of-final-grade",
    "href": "syllabus.html#app-projects-27-of-final-grade",
    "title": "Syllabus",
    "section": "📂 App Projects (27% of final grade)",
    "text": "📂 App Projects (27% of final grade)\n\nThere will be about 3 Shiny App projects.\nThese are larger assignments intended to combine many of the ideas from the course, in order to create interactive web-apps.\nAs part of the submission you will have to record a video (with screen and face capture) in which you describe how to use your app, and explain the performed analysis and some of the obtained results.\nStarting on week-8 app projects will be assigned about every two weeks, until the end of instruction.\nWe will not drop any of the App assignment scores in the calculation of your overall grade."
  },
  {
    "objectID": "syllabus.html#late-policy-and-hw-assignment-extensions",
    "href": "syllabus.html#late-policy-and-hw-assignment-extensions",
    "title": "Syllabus",
    "section": "🕚 Late Policy and HW Assignment Extensions",
    "text": "🕚 Late Policy and HW Assignment Extensions\nIf you cannot turn in a HW assignment on time, our default policy is:\n\nSubmissions within 24 hours after the deadline will receive a 15% deduction.\nSubmissions within 48 hours after the deadline will receive a 30% deduction.\nSubmissions that are 48 hours or more after the deadline will receive no credit.\n\nRequesting an extension: If you need to request an extension, regardless of your DSP status, fill out this google form. Submissions to this form will be visible only to the course staff members.\n\nAny first-time request for a 1-day extension on a problem set assignment, made before an assignment’s deadline, will be guaranteed to be approved, so long as it is made in good faith.\nAny first-time request for a 1-day extension on a shiny app assignment, made before an assignment’s deadline, will be guaranteed to be approved, so long as it is made in good faith.\nAny occasional request for a 2-day extension (requested before an assignment’s deadline) made by a student with a DSP accommodation for assignment extensions will be approved automatically.\n\nAlso, please keep in mind that we are dropping the lowest score of your six Problem-set assignments (do not confuse with App projects). This policy is in place to take care of any extenuating circumstances that prevent you from submitting one of these assignments.\nPlease plan ahead and pace yourself. Don’t wait until the last day to do an assignment. Don’t wait until the last minute to submit your assignments."
  },
  {
    "objectID": "syllabus.html#midterm-7-of-final-grade",
    "href": "syllabus.html#midterm-7-of-final-grade",
    "title": "Syllabus",
    "section": "📝 Midterm (7% of final grade)",
    "text": "📝 Midterm (7% of final grade)\n\nThere will be one midterm assigned on Wed Mar-5th.\nMore information about the midterm will be announced as we approach its due date.\nUnless you have approved accommodations, we won’t be able to provide any extensions if you miss the midterm."
  },
  {
    "objectID": "syllabus.html#final-exam-21-of-final-grade",
    "href": "syllabus.html#final-exam-21-of-final-grade",
    "title": "Syllabus",
    "section": "📝 Final Exam (21% of final grade)",
    "text": "📝 Final Exam (21% of final grade)\n\nThere will be one final exam.\nThe final exam will be on Wed May-14th (7pm-10pm), as scheduled by the University.\nMore information about the final exam will be provided as we approach its due date.\nUnless you have accommodations as determined by the university and approved by the instructor, you must take the exam at the date and times provided here.\nPlease check your course schedule and make sure that you can take the final exam on the scheduled date. Otherwise, do not take the class if you are not available at this date."
  },
  {
    "objectID": "syllabus.html#grading-structure",
    "href": "syllabus.html#grading-structure",
    "title": "Syllabus",
    "section": "💯 Grading Structure",
    "text": "💯 Grading Structure\nGrades will be assigned using the following weighted components:\n\n10% Lab (drop 2 lowest scores)\n35% Psets (drop lowest score)\n8% App1 (no drop)\n9% App2 (no drop)\n10% App3 (no drop)\n7% Midterm\n21% Final Exam\n\n\nTo complete the course, you must take the final exam.\nTo try to keep grading consistent across semesters, I may occasionally curve an individual assignment. I will do this only if I made the assignment or exam harder than I intended, and no one’s scores will decrease as a result of the curve. This will happen very rarely, probably not at all.\nTo give you a rough idea of the grading scheme, the assignment of letter grades in previous semesters has been as follows:\n\n😀 90-100% (Excellent) A-/A/A+ range\n🙂 80-90% (Good) B-/B/B+ range\n😐 70-80% (Fair) C-/C/C+ range\n🙁 60-70% (Deficient) D\n😞 Below 60% (Failed) F\n\nIf you are taking the class pass-fail, the cut-oﬀ for passing is 70% (C-).\nAs a matter of course policy, I do not round up when calculating letter grades. Ex: if your overall score is 79.9999%, then the highest letter grade that you can expect is a C+, not a B-.\nThere is no curve; your grade will depend only on how well you do, and not on how well everyone else does.\nLetter grades are final; I don’t enter into negotiations with students about grades.\nPlease do not engage in grade grubbing.\nAlso, please remember that we grade your course performance, not your personal worth."
  },
  {
    "objectID": "syllabus.html#generative-a.i.-policy",
    "href": "syllabus.html#generative-a.i.-policy",
    "title": "Syllabus",
    "section": "⚠️ Generative A.I. Policy",
    "text": "⚠️ Generative A.I. Policy\nGenerative A.I. refers to artificial intelligence technologies, like those used for ChatGPT and similar, that can draw on a large corpus of training data to create new written, visual, or audio content.\nThere are two principles we use to guide our class policy on AI use:\n\nCognitive dimension: Working with AI should not reduce your ability to think clearly. The use of AI should facilitate—rather than hinder—learning.\nEthical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\nIn this course, we’ll be developing skills that are important to practice on your own. Because use of generative A.I. may inhibit the development of those skills, the use of these tools is permitted in this course for the following activities:\n\nBrainstorming and refining your ideas;\nFine tuning your exploratory/research questions;\nDrafting an outline to organize your thoughts;\nChecking syntax errors or bugs in your code; and\nPolishing your spelling and grammar.\n\nThe use of generative A.I. tools is not permitted in this course for the following activities:\n\nImpersonating you in classroom contexts, such as by using the tool to compose discussion board prompts assigned to you or content that you put into a discussion forum/chat.\nAttempting to pass off AI-generated work as your own.\nWriting a draft of your assignment.\nWriting entire blocks of code, functions, or scripts to complete class assignments.\n\nPlease keep in mind that use of generative A.I. tools can impede your learning by generating ideas for you before you had a chance to think of your own ideas; inhibiting the development of your own writing skills; generating factually inaccurate statements or fictional reference sources; etc.\nIf you are unsure of whether and how much of a submission has been AI-generated, or whether you are in violation of a certain policy, please reach out to us and ask for guidance."
  },
  {
    "objectID": "syllabus.html#academic-honesty",
    "href": "syllabus.html#academic-honesty",
    "title": "Syllabus",
    "section": "☝️ Academic Honesty",
    "text": "☝️ Academic Honesty\nYou should not share your code or answers, directly or indirectly, with other students. Doing so doesn’t help them; it just sets them up for trouble on exams. Feel free to discuss the problems with others beforehand, but not the solutions. Please complete your own work and keep it to yourself (e.g. avoid sharing it in hosting platforms like Github or similar). If you suspect other people may be plagiarizing you, let us know ASAP.\nWe expect you to do your own work and to uphold the standards of intellectual integrity. Collaborating on homework is fine and we encourage you to work together—but copying is not, nor is having somebody else submit assignments for you. Likewise, obtaining and/or using solutions from previous years or from the internet, if such happen to be available, is considered cheating.\nBeyond the templates or starting code provided by the teaching staff, any writing, code, media, or other submissions not explicitly identified as AI-generated will be assumed as original to the student. Submitting AI-generated work without identifying it as such will be considered a violation of the Code of Student Conduct.\nCheating will not be tolerated. Any evidence of academic misconduct will result in a score of zero (0) on the entire assignment or examination, and a failing letter grade. We will always report incidences of cheating to the Center for Student Conduct.\nIf you are having trouble with an assignment or studying for an exam, or if you are uncertain about permissible and impermissible conduct or collaboration, please contact us.\nRather than copying someone else’s work, ask for help. You are not alone in this course! The course staff is here to help you succeed. If you invest the time to learn the material and complete the projects, you won’t need to copy any answers."
  },
  {
    "objectID": "syllabus.html#email-policy",
    "href": "syllabus.html#email-policy",
    "title": "Syllabus",
    "section": "✉️ Email Policy",
    "text": "✉️ Email Policy\nIf you wish for your email to make it into our inbox, the subject of your email must contain the text: Stat 133.\nPlease refer to my email guidelines for more information: communication via email"
  },
  {
    "objectID": "syllabus.html#special-accommodations",
    "href": "syllabus.html#special-accommodations",
    "title": "Syllabus",
    "section": "🚸 Special Accommodations",
    "text": "🚸 Special Accommodations\nStudents needing accommodations for any physical, psychological, or learning disability, should contact the teaching staff during the first two weeks of the semester, and see http://dsp.berkeley.edu to learn about Berkeley’s policy. If you are a DSP student, please contact us at least three weeks prior to a midterm or final so that we can work out acceptable accommodations.\nFor relevant DSP accommodations that provide occasional extensions on assignments, please see the above Late Policy."
  },
  {
    "objectID": "syllabus.html#incomplete-grade",
    "href": "syllabus.html#incomplete-grade",
    "title": "Syllabus",
    "section": "❗Incomplete Grade",
    "text": "❗Incomplete Grade\nUnder emergency/special circumstances, students may petition me to receive an Incomplete grade. By University policy, for a student to get an Incomplete requires (i) that the student was performing passing-level work until the time that (ii) something happened that—through no fault of the student—prevented the student from completing the coursework. If you take the final, you completed the course, even if you took it while ill, exhausted, mourning, etc. The time to talk to me about incomplete grades is BEFORE you take the final (several weeks before), when the situation that prevents you from finishing the course presents itself. Please clearly state your reasoning in your comments to me.\nIt is your responsibility to develop good time management skills, good studying habits, know your limits, and learn to ask for professional help. Life happens. Social, family, cultural, scholar, and individual circumstances can affect your performance (both positive and negatively). If you find yourself in a situation that raises concerns about passing the course, please contact me as soon as possible.\nAbove all, please-please-please do not wait till the end of the semester to share your concerns about passing the course because it will be too late by then."
  },
  {
    "objectID": "syllabus.html#safe-and-inclusive-environment",
    "href": "syllabus.html#safe-and-inclusive-environment",
    "title": "Syllabus",
    "section": "🌻 Safe and Inclusive Environment",
    "text": "🌻 Safe and Inclusive Environment\nWhenever a faculty member, staff member, post-doc, or GSI is responsible for the supervision of a student, a personal relationship between them of a romantic or sexual nature, even if consensual, is against university policy. Any such relationship jeopardizes the integrity of the educational process.\nAlthough faculty and staff can act as excellent resources for students, you should be aware that they are required to report any violations of this campus policy. If you wish to have a confidential discussion on matters related to this policy, you may contact the Confidential Care Advocates on campus for support related to counseling or sensitive issues. Appointments can be made by calling (510) 642-1988.\nThe classroom, lab, and work place should be safe and inclusive environments for everyone. The Office for the Prevention of Harassment and Discrimination (OPHD) is responsible for ensuring the University provides an environment for faculty, staff and students that is free from discrimination and harassment on the basis of categories including race, color, national origin, age, sex, gender, gender identity, and sexual orientation. Questions or concerns? Call (510) 643-7985, email ask_ophd@berkeley.edu, or go to https://svsh.berkeley.edu/."
  },
  {
    "objectID": "syllabus.html#last-but-not-least",
    "href": "syllabus.html#last-but-not-least",
    "title": "Syllabus",
    "section": "🎉 Last But Not Least",
    "text": "🎉 Last But Not Least\nThe main goal of STAT 133 is that you should learn, and have a fantastic experience doing so. Please keep that goal in mind throughout the semester."
  },
  {
    "objectID": "02-datatypes/index.html",
    "href": "02-datatypes/index.html",
    "title": "Data Types",
    "section": "",
    "text": "Read Inferential Thinking\n\n\n\nRead Ch 4 intro and Ch 4.1, which describes in detail how Python evaluates expressions involving numeric data types.\nBefore continuing, make sure you understand the following:\n\nEvery value has a type (data type), and the built-in type function returns the type of the result of any expression.\nIn Python, integers are called int values. Real numbers are called float values. These are flexible but have some computational limitations.\nThe type of an expression is the type of its final value.\nWhen a float value is combined with an int value using some arithmetic operator, then the result is always a float value.\n\n\n\n\n\nYou have seen this table before.\n\nCommon Python operators for numeric data types\n\n\n\n\n\n\n\n\nOperator\nSymbol\nExample Expression\nExpression Value\n\n\n\n\nAddition\n+\n2 + 3\n5\n\n\nSubtraction\n-\n15 - 4\n11\n\n\nMultiplication\n*\n-2 * 9\n-18\n\n\nDivision\n/\n15 / 2\n7.5\n\n\nInteger division\nCuts off remainder\n//\n15 // 2\n\n\nRemainder/Modulo\n%\n19 % 3\n1  (19 ÷ 3 = 6 Remainder 1)\n\n\nExponentiation\n**\n3 ** 2\n9",
    "crumbs": [
      "L02: Data Types"
    ]
  },
  {
    "objectID": "02-datatypes/index.html#numeric-data-types",
    "href": "02-datatypes/index.html#numeric-data-types",
    "title": "Data Types",
    "section": "",
    "text": "Read Inferential Thinking\n\n\n\nRead Ch 4 intro and Ch 4.1, which describes in detail how Python evaluates expressions involving numeric data types.\nBefore continuing, make sure you understand the following:\n\nEvery value has a type (data type), and the built-in type function returns the type of the result of any expression.\nIn Python, integers are called int values. Real numbers are called float values. These are flexible but have some computational limitations.\nThe type of an expression is the type of its final value.\nWhen a float value is combined with an int value using some arithmetic operator, then the result is always a float value.\n\n\n\n\n\nYou have seen this table before.\n\nCommon Python operators for numeric data types\n\n\n\n\n\n\n\n\nOperator\nSymbol\nExample Expression\nExpression Value\n\n\n\n\nAddition\n+\n2 + 3\n5\n\n\nSubtraction\n-\n15 - 4\n11\n\n\nMultiplication\n*\n-2 * 9\n-18\n\n\nDivision\n/\n15 / 2\n7.5\n\n\nInteger division\nCuts off remainder\n//\n15 // 2\n\n\nRemainder/Modulo\n%\n19 % 3\n1  (19 ÷ 3 = 6 Remainder 1)\n\n\nExponentiation\n**\n3 ** 2\n9",
    "crumbs": [
      "L02: Data Types"
    ]
  },
  {
    "objectID": "02-datatypes/index.html#strings",
    "href": "02-datatypes/index.html#strings",
    "title": "Data Types",
    "section": "Strings",
    "text": "Strings\n\n\n\n\n\n\nRead Inferential Thinking\n\n\n\nRead Ch 4.2 which describes the string data type.\nBefore continuing, make sure you understand the following:\n\nA string is a text data type. It can use single quotes or double quotes.\nThe meaning of an expression depends both upon its structure and the types of values that are being combined.\n\n\n\nConcatenation operation: The + operator works differently on string data types. Instead of adding numerically, it “adds textually,” which is more formally called concatenation:\n\n2 + 3  # addition\n\n5\n\n\n\n'hello' + \"donuts\"\n\n'hellodonuts'\n\n\nLength function: There is one function not shown above that would be useful to you know, and that is len(s), which takes a string argument and returns its length.\n\ns = \"hello world\"\nlen(s)\n\n11",
    "crumbs": [
      "L02: Data Types"
    ]
  },
  {
    "objectID": "02-datatypes/index.html#boolean-data-type",
    "href": "02-datatypes/index.html#boolean-data-type",
    "title": "Data Types",
    "section": "Boolean Data Type",
    "text": "Boolean Data Type\nThe Boolean data type (bool) has exactly two values: True and False. Note that boolean values are not strings!\n\nb = True\nb\n\nTrue\n\n\n\nb = False\ntype(b)\n\nbool\n\n\nIn our current view of Python as an advanced calculator, it may be unclear why such a special data type is needed. We will find soon that it is very useful to have a special data type to represent whether something is true or false.",
    "crumbs": [
      "L02: Data Types"
    ]
  },
  {
    "objectID": "02-datatypes/index.html#typecasting",
    "href": "02-datatypes/index.html#typecasting",
    "title": "Data Types",
    "section": "Typecasting",
    "text": "Typecasting\nWe can also typecast, or convert values between data types. These typecasting functions take in one typed argument and return another typed argument, then return that value as a different type. The function name is generally the type. Note that data type conversion is only valid “when it makes sense.” We’ll talk about this more later.\n\nx = 3       # what type is x?\nstr(x)      # returns a value of type string\n\n'3'\n\n\n\ns = \"5\"       # what type is x?\nfloat(s)      # what type is returned?\n\n5.0",
    "crumbs": [
      "L02: Data Types"
    ]
  },
  {
    "objectID": "02-datatypes/index.html#external-reading",
    "href": "02-datatypes/index.html#external-reading",
    "title": "Data Types",
    "section": "External Reading",
    "text": "External Reading\n\n(mentioned in notes) Computational and Inferential Thinking, Ch 4 intro, Ch 4.1, Ch 4.2\n(optional) Tomas Beuzen. Python Programming for Data Science Ch 1.2.",
    "crumbs": [
      "L02: Data Types"
    ]
  },
  {
    "objectID": "03-arrays/index.html",
    "href": "03-arrays/index.html",
    "title": "Arrays",
    "section": "",
    "text": "This note has the following goals:\nWe will first learn arrays through analyzing data; then, we will drill into array internals and the nitty-gritty of Python modules and methods. Finally, we will summarize array operations.",
    "crumbs": [
      "L03: Arrays"
    ]
  },
  {
    "objectID": "03-arrays/index.html#definitions",
    "href": "03-arrays/index.html#definitions",
    "title": "Arrays",
    "section": "Definitions",
    "text": "Definitions\nLet us discuss our first data structure in this course: arrays. An array is a sequential collection of values of a given data type:\n\nsequential: arranged like a line/queue\ncollection: multiple values organized together.\n\nIn arrays, each value is called an array element.\nWe have previously discussed the idea of tables: a rectangular data structure with rows and columns. We will see today that arrays are a concise way to manipulate table columns, because arrays facilitate common data processing that we may want to perform on columns.\n\n\n\n\n\n\nRead Inferential Thinking\n\n\n\nRead Ch 5.1, which describes in detail how arrays can be used in arithmetic expressions to compute over their contents.\nBefore continuing, make sure that:\n\nYou understand the figure that shows how to convert an array of Celsius temperatures to an array of Farenheit temperatures.\n\n\n\n\nToday’s dataset\nThe following table is drawn from the American Community Survey (ACS) of 2020. It shows education levels of adults 25 years or higher by state.\n\n\n\n\n\n\n\n\n\nState\nEstimated total state population\nEstimated high school graduate or higher (%)\nEstimated bachelor’s degree or higher (%)\n\n\n\n\nAlabama\n3,344,006\n86.9\n26.2\n\n\nCalifornia\n26,665,143\n83.9\n34.7\n\n\nFlorida\n15,255,326\n88.5\n30.5\n\n\nNew York\n13,649,157\n87.2\n37.5\n\n\nTexas\n18,449,851\n84.4\n30.7",
    "crumbs": [
      "L03: Arrays"
    ]
  },
  {
    "objectID": "03-arrays/index.html#creating-arrays",
    "href": "03-arrays/index.html#creating-arrays",
    "title": "Arrays",
    "section": "Creating arrays",
    "text": "Creating arrays\nEach of these table columns can be represented by an array.\nBelow, we create a new array for the column “Estimated high school graduate or higher (%)” and assign the returned array to a single name, hs_or_higher. This simple assignment statement is abstraction at work! Also note that the import statement gives us access to array functions with the datascience module, including make_array, which returns a new array with the provided argument values.\n\nfrom datascience import *\n\nhs_or_higher = make_array(86.9, 83.9, 88.5, 87.2, 84.4)\nhs_or_higher\n\narray([ 86.9,  83.9,  88.5,  87.2,  84.4])\n\n\nLet’s make a few more arrays.\nThe array data type (as shown below) is a bit esoteric for now; we will discuss what NumPy (np) is very soon.\n\nbs_or_higher = make_array(26.2, 34.7, 30.5, 37.5, 30.7)\ntype(bs_or_higher)\n\nnumpy.ndarray\n\n\nWhen creating the state names array of strings below, what do you observe about the datatype, dtype? Hint: Count the number of characters in each string.\n\nstates = make_array(\"Alabama\", \"California\", \"Florida\", \"New York\", \"Texas\")\nstates\n\narray(['Alabama', 'California', 'Florida', 'New York', 'Texas'],\n      dtype='&lt;U10')\n\n\nWhen creating the state population array below, why might we decide to make an integer array, as opposed to a string array?\n\nstate_pop = make_array(3344006, 26665143, 15255326, 13649157, 18449851)\nstate_pop\n\narray([ 3344006, 26665143, 15255326, 13649157, 18449851])\n\n\n\n\n\n\n\n\nMore array details\n\n\n\n\n\nThe order of an array is fixed (i.e., they will be arranged in the order specified when building the array), and values can be repeated.\nArray with 4 ints:\n\nmake_array(5, -1, 0.3, 5)\n\narray([ 5. , -1. ,  0.3,  5. ])\n\n\nValues in an array must all be of the same data type, and the make_array function will cast appropriately. Below, all values can be represented by strings:\n\nmake_array(4, -4.5, \"not a number\")\n\narray(['4', '-4.5', 'not a number'],\n      dtype='&lt;U32')\n\n\nIncidentally, we can clean up our code stylistically by makine line breaks after each argument:\n\nmake_array(\"hello\",\n           \"world\",\n           \"!\")\n\narray(['hello', 'world', '!'],\n      dtype='&lt;U5')",
    "crumbs": [
      "L03: Arrays"
    ]
  },
  {
    "objectID": "03-arrays/index.html#element-wise-arithmetic",
    "href": "03-arrays/index.html#element-wise-arithmetic",
    "title": "Arrays",
    "section": "Element-wise arithmetic",
    "text": "Element-wise arithmetic\nArrays allow us to write code that performs the same operation on many pieces of data at once. We can therefore easily use arithmetic operations on elements of numeric arrays where it “makes sense” (see sidenote).\nTo compute the estimated percentage by state of adults 25 years or higher that have not graduated high school, we can create a new array by performing arithmetic with an array and a numeric value:\n\n100 - hs_or_higher\n\narray([ 13.1,  16.1,  11.5,  12.8,  15.6])\n\n\nTo compute the estimated number by state of adults 25 years or higher with bachelor’s degrees, we can create a new array by performing arithmetic with two arrays:\n\nbs_or_higher / 100 * state_pop\n\narray([  876129.572,  9252804.621,  4652874.43 ,  5118433.875,  5664104.257])\n\n\nSidenote: What do I mean by “makes sense”? Linear Algebra is a broad mathematical field that forms the foundations of much of data science and tabular data analysis. This element-wise array functionality is derived from the mathematical definitions of vectors and scalars. Take a linear algebra class if you want to learn more!",
    "crumbs": [
      "L03: Arrays"
    ]
  },
  {
    "objectID": "03-arrays/index.html#indexing",
    "href": "03-arrays/index.html#indexing",
    "title": "Arrays",
    "section": "Indexing",
    "text": "Indexing\nWhen people stand in a line, each person has a position. Similarly, each element (i.e., value) of an array has a position – called its index. Python, like many programming languages, is zero-indexed. This means that in an array, the first element has index 0, not 1.\nIn the int_arr array below, the first element (3) has index 0; the last element (2) has index 4.\n\nint_arr = make_array(3, -4, 0, 5, 2)\nint_arr\n\narray([ 3, -4,  0,  5,  2])\n\n\n\nThe Array Method item()\nAn array method is just like a function, but it must operate on an array using “dot” syntax. So the call looks like:\nname_of_array.method(arguments)\nWe will discuss many more methods once we introduce tables, but for now let’s learn our first method to index arrays.\nWe can access an element in an array by using its index and the item() method:\n\nint_arr.item(0)\n\n3\n\n\n\nint_arr.item(3)\n\n5\n\n\nBecause of zero-indexing, the largest valid index is 4 for the five-element int_arr array:\n\nint_arr.item(5)\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[13], line 1\n----&gt; 1 int_arr.item(5)\n\nIndexError: index 5 is out of bounds for axis 0 with size 5\n\n\n\nIn Python, we can also “count backwards” using negative indexes. -1 corresponds to the last element in a array; -2 corresponds to the second to last element in a array; and so on.\n\n# functionally equivalent to int_arr.item(4)\nint_arr.item(-1)\n\n2",
    "crumbs": [
      "L03: Arrays"
    ]
  },
  {
    "objectID": "03-arrays/index.html#external-reading",
    "href": "03-arrays/index.html#external-reading",
    "title": "Arrays",
    "section": "External Reading",
    "text": "External Reading\n\n(mentioned in notes) Computational and Inferential Thinking, Ch 5.1\n(optional) Tomas Beuzen. Python Programming for Data Science Ch 1.2.",
    "crumbs": [
      "L03: Arrays"
    ]
  },
  {
    "objectID": "03-arrays/index.html#references",
    "href": "03-arrays/index.html#references",
    "title": "Arrays",
    "section": "References",
    "text": "References\nU.S. Census Bureau, “EDUCATIONAL ATTAINMENT,” American Community Survey 5-Year Estimates Subject Tables, Table S1501, 2020, https://data.census.gov/table/ACSST5Y2020.S1501?q=2020+education&t=Age+and+Sex:Educational+Attainment&g=010XX00US$0400000, accessed on August 24, 2025.",
    "crumbs": [
      "L03: Arrays"
    ]
  }
]